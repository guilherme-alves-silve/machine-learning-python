{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b1f0d9-86d2-42c2-b884-c8a63a4da1ef",
   "metadata": {},
   "source": [
    "# [Curso de aprendizagem por refor√ßo](https://www.udemy.com/course/inteligencia-artificial-empresas-negocios/): Q-Learning\n",
    "\n",
    "![caso_1.jpg](caso_1.jpg)\n",
    "\n",
    "## State Transition Matrix - Markov Chain\n",
    "|  | A| B| C| D| E| F| G| H| I| J| K| L|\n",
    "|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    "| A| 0| 1| 0| 0| 0| 0| 0| 0| 0| 0| 0| 0|\n",
    "| B| 1| 0| 1| 0| 0| 1| 0| 0| 0| 0| 0| 0|\n",
    "| C| 0| 1| 0| 0| 0| 0| 1| 0| 0| 0| 0| 0|\n",
    "| D| 0| 0| 0| 0| 0| 0| 0| 1| 0| 0| 0| 0|\n",
    "| E| 0| 0| 0| 0| 0| 0| 0| 0| 1| 0| 0| 0|\n",
    "| F| 0| 1| 0| 0| 0| 0| 0| 0| 0| 1| 0| 0|\n",
    "| G| 0| 0| 1| 0| 0| 0| 0| 1| 0| 0| 0| 0|\n",
    "| H| 0| 0| 0| 1| 0| 0| 1| 0| 0| 0| 0| 1|\n",
    "| I| 0| 0| 0| 0| 1| 0| 0| 0| 0| 1| 0| 0|\n",
    "| J| 0| 0| 0| 0| 0| 1| 0| 0| 1| 0| 1| 0|\n",
    "| K| 0| 0| 0| 0| 0| 0| 0| 0| 0| 1| 0| 1|\n",
    "| L| 0| 0| 0| 0| 0| 0| 0| 1| 0| 0| 1| 0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d38a76d-62cd-4e05-b134-a4050d9b526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "from collections import namedtuple\n",
    "\n",
    "Space = namedtuple('Space', ['n', 'sample'])\n",
    "\n",
    "\n",
    "class Env:\n",
    "\n",
    "    def __init__(self, start_location: str, end_location: str, debug=False):\n",
    "        assert isinstance(start_location, str), \"start_location must be str\"\n",
    "        assert isinstance(end_location, str), \"end_location must be str\"\n",
    "        assert isinstance(debug, bool), \"debug must be bool\"\n",
    "        self.debug = debug\n",
    "        self.action_space = Space(n=12, sample=lambda: np.random.choice(self._possible_states()))\n",
    "        self.state_space = Space(n=12, sample=None)\n",
    "        self.location_to_state = {'A': 0,\n",
    "                                  'B': 1,\n",
    "                                  'C': 2,\n",
    "                                  'D': 3,\n",
    "                                  'E': 4,\n",
    "                                  'F': 5,\n",
    "                                  'G': 6,\n",
    "                                  'H': 7,\n",
    "                                  'I': 8,\n",
    "                                  'J': 9,\n",
    "                                  'K': 10,\n",
    "                                  'L': 11}\n",
    "        self.state_to_location = {state: location for location, state in self.location_to_state.items()}\n",
    "        self.start = self.location_to_state[start_location]\n",
    "        self.end = self.location_to_state[end_location]\n",
    "        self.original_R = np.array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                                    [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                                    [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                                    [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]])\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action: Union[int, str]):\n",
    "        assert action in self.location_to_state or action in self.state_to_location,\\\n",
    "            f\"Invalid action: {action}\"\n",
    "        if isinstance(action, str):\n",
    "            action = self.location_to_state[action]\n",
    "\n",
    "        possible_states = self._possible_states()\n",
    "        assert action in possible_states, f\"action not possible. Must choose: {possible_states}\"\n",
    "        next_state: int = action\n",
    "        reward = self.R[self.curr_state, action]\n",
    "        term = action == self.end and self.curr_state == self.end\n",
    "        self.curr_state = next_state\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"next_state:\", next_state, \", reward:\", reward, \", term:\", term)\n",
    "            print(\"possible next_state:\", self._states_to_locations(next_state))\n",
    "        self.curr_state = action\n",
    "        return next_state, reward, term\n",
    "\n",
    "    def _possible_states(self):\n",
    "        possible_states = np.flatnonzero(self.R[self.curr_state,:])\n",
    "        return possible_states\n",
    "    \n",
    "    def _states_to_locations(self, state: int):\n",
    "        return [self.state_to_location[state] for state in np.flatnonzero(self.R[state,:])]\n",
    "\n",
    "    def reset(self):\n",
    "        self.R = np.copy(self.original_R)\n",
    "        self.R[self.end, self.end] = 100\n",
    "        self.curr_state = self.start\n",
    "        return self.curr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98534bd2-ddeb-41bd-af15-839fa402ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.75\n",
    "ALPHA = 0.9\n",
    "REDUCE_EPSILON = 0.99995\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c492d176-f06d-49c9-ac5f-c93780880b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection_epsilon_greedy(env: Env, q_table: np.array, state: int, epsilon: float):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    return action\n",
    "\n",
    "def compute_next_q_value(old_q_value: float, reward: float, optimal_q_value: float):\n",
    "    return old_q_value + ALPHA * (reward + GAMMA * optimal_q_value - old_q_value)\n",
    "\n",
    "def reduce_epsilon(epsilon: float):\n",
    "    return epsilon * REDUCE_EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41a806f9-a769-4ec7-b681-30946b124b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route: \n",
      "['A', 'B', 'C', 'G', 'H', 'L']\n",
      "['E', 'I', 'J', 'K', 'L', 'H', 'G']\n"
     ]
    }
   ],
   "source": [
    "def route(start_location: str, end_location: str, debug=False) -> list:\n",
    "    epsilon = REDUCE_EPSILON\n",
    "    env = Env(start_location, end_location, debug=False)\n",
    "    q_table = np.zeros((env.state_space.n, env.action_space.n))\n",
    "    for epoch in range(EPOCHS):\n",
    "        state = env.reset()\n",
    "        term = False\n",
    "        while not term:\n",
    "            action = action_selection_epsilon_greedy(env, q_table, state, epsilon)\n",
    "            next_state, reward, term = env.step(action)\n",
    "            old_q_value = q_table[state, action]\n",
    "            next_optimal_q_value = np.max(q_table[next_state,:])\n",
    "            next_q_value = compute_next_q_value(old_q_value, reward, next_optimal_q_value)\n",
    "            q_table[state, action] = next_q_value\n",
    "            state = next_state\n",
    "        epsilon = reduce_epsilon(epsilon)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Q-Table\")\n",
    "        print(q_table)\n",
    "\n",
    "    curr_loc = start_location\n",
    "    path = []\n",
    "    path.append(curr_loc)\n",
    "    state = env.reset()\n",
    "    while curr_loc != end_location:\n",
    "        action = action_selection_epsilon_greedy(env, q_table, state, epsilon=-1)\n",
    "        next_state, reward, term = env.step(action)\n",
    "        curr_loc = env.state_to_location[next_state]\n",
    "        path.append(curr_loc)\n",
    "        state = next_state\n",
    "    return path\n",
    "\n",
    "def best_route(start_location: str, intermediate_location: str, end_location: str) -> list:\n",
    "    return route(start_location, intermediate_location)[:-1] + route(intermediate_location, end_location)\n",
    "\n",
    "print('Route: ')\n",
    "print(route('A', 'L'))\n",
    "print(best_route('E', 'K', 'G'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
