{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDPcCMXIY8GO"
   },
   "source": [
    "# Study of Reinforcement Learning based on the [course](https://www.udemy.com/course/practical-ai-with-python-and-reinforcement-learning)\n",
    "\n",
    "## Libraries\n",
    "\n",
    "- [Gym](https://github.com/openai/gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TEkbn6LZHU8"
   },
   "source": [
    "## Deep Q-Learning (DQN) with Keras-RL2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcAeMwv5Y3bV",
    "outputId": "743e4405-4a0d-4582-818f-8bf0ac6ced0f"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.1.0 gym[classic_control]==0.17.3 matplotlib numpy keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaABXf-hZ1Tj",
    "outputId": "743c2972-49f6-4596-b713-9504bb520f58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "peT_1l71Z1tl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import logger as gymlogger\n",
    "from IPython.display import clear_output\n",
    "\n",
    "gymlogger.set_level(40) # error only\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5bJDK_y7ahgk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_environment(array):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(environment)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "g1_UwZexa6-Z"
   },
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "3MfvVokHbEmE",
    "outputId": "5467dab0-f118-4813-8272-137dbfc5f8c3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for step in range(50):\n",
    "  environment = env.render(mode=\"rgb_array\")\n",
    "  show_environment(environment)\n",
    "  action = env.action_space.sample()\n",
    "  env.step(action)\n",
    "  clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "VrsRO2NqbRDm",
    "outputId": "ee0bb40c-5708-4e26-ba0d-8e6c9271eaec"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5zuy0IBacegu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,) + num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4Me0pdFZclEP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programas\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + num_states))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Dc2F3NVicpFu"
   },
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "\n",
    "memory = SequentialMemory(limit=20_000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "luqWj_WPcpH5"
   },
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr=\"eps\",\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bv47Lf9KcpKj"
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
    "               memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=100, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YD_f_1h8cpNJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programas\\miniconda3\\envs\\myenv\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    17/20000: episode: 1, duration: 1.367s, episode steps:  17, steps per second:  12, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.755139, mae: 0.680204, mean_q: 0.110727, mean_eps: 0.999392\n",
      "    40/20000: episode: 2, duration: 0.096s, episode steps:  23, steps per second: 240, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.515269, mae: 0.565441, mean_q: 0.164575, mean_eps: 0.998740\n",
      "    52/20000: episode: 3, duration: 0.050s, episode steps:  12, steps per second: 240, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.346479, mae: 0.535987, mean_q: 0.297736, mean_eps: 0.997952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programas\\miniconda3\\envs\\myenv\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    76/20000: episode: 4, duration: 0.104s, episode steps:  24, steps per second: 230, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.195563, mae: 0.518587, mean_q: 0.474091, mean_eps: 0.997142\n",
      "   102/20000: episode: 5, duration: 0.106s, episode steps:  26, steps per second: 245, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.096370, mae: 0.540950, mean_q: 0.728052, mean_eps: 0.996018\n",
      "   137/20000: episode: 6, duration: 0.141s, episode steps:  35, steps per second: 247, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.236672, mae: 0.958295, mean_q: 1.352800, mean_eps: 0.994645\n",
      "   152/20000: episode: 7, duration: 0.061s, episode steps:  15, steps per second: 245, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.077866, mae: 1.055834, mean_q: 1.983098, mean_eps: 0.993520\n",
      "   170/20000: episode: 8, duration: 0.074s, episode steps:  18, steps per second: 245, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.066880, mae: 1.037676, mean_q: 2.032595, mean_eps: 0.992777\n",
      "   189/20000: episode: 9, duration: 0.081s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.053902, mae: 1.015159, mean_q: 1.987113, mean_eps: 0.991945\n",
      "   203/20000: episode: 10, duration: 0.060s, episode steps:  14, steps per second: 234, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.152107, mae: 1.074958, mean_q: 1.994525, mean_eps: 0.991202\n",
      "   213/20000: episode: 11, duration: 0.045s, episode steps:  10, steps per second: 222, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.503636, mae: 1.585603, mean_q: 2.373837, mean_eps: 0.990663\n",
      "   236/20000: episode: 12, duration: 0.105s, episode steps:  23, steps per second: 218, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.191233, mae: 1.657230, mean_q: 3.225849, mean_eps: 0.989920\n",
      "   246/20000: episode: 13, duration: 0.048s, episode steps:  10, steps per second: 210, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.180755, mae: 1.636793, mean_q: 3.291432, mean_eps: 0.989177\n",
      "   258/20000: episode: 14, duration: 0.055s, episode steps:  12, steps per second: 218, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.120832, mae: 1.589730, mean_q: 3.200650, mean_eps: 0.988682\n",
      "   275/20000: episode: 15, duration: 0.078s, episode steps:  17, steps per second: 217, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.228766, mae: 1.650799, mean_q: 3.295963, mean_eps: 0.988030\n",
      "   317/20000: episode: 16, duration: 0.194s, episode steps:  42, steps per second: 216, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.353548, mae: 1.835527, mean_q: 3.373937, mean_eps: 0.986703\n",
      "   345/20000: episode: 17, duration: 0.124s, episode steps:  28, steps per second: 227, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.389267, mae: 2.252252, mean_q: 4.424578, mean_eps: 0.985127\n",
      "   354/20000: episode: 18, duration: 0.041s, episode steps:   9, steps per second: 221, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.361875, mae: 2.238035, mean_q: 4.289546, mean_eps: 0.984295\n",
      "   385/20000: episode: 19, duration: 0.132s, episode steps:  31, steps per second: 235, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.251288, mae: 2.220528, mean_q: 4.367148, mean_eps: 0.983395\n",
      "   396/20000: episode: 20, duration: 0.047s, episode steps:  11, steps per second: 232, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.313808, mae: 2.257968, mean_q: 4.390326, mean_eps: 0.982450\n",
      "   417/20000: episode: 21, duration: 0.088s, episode steps:  21, steps per second: 239, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.689945, mae: 2.624407, mean_q: 4.682499, mean_eps: 0.981730\n",
      "   436/20000: episode: 22, duration: 0.076s, episode steps:  19, steps per second: 250, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.419661, mae: 2.798641, mean_q: 5.482220, mean_eps: 0.980830\n",
      "   455/20000: episode: 23, duration: 0.092s, episode steps:  19, steps per second: 206, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.449875, mae: 2.754624, mean_q: 5.266154, mean_eps: 0.979975\n",
      "   468/20000: episode: 24, duration: 0.066s, episode steps:  13, steps per second: 197, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.475544, mae: 2.733224, mean_q: 5.263539, mean_eps: 0.979255\n",
      "   497/20000: episode: 25, duration: 0.157s, episode steps:  29, steps per second: 184, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.447043, mae: 2.742441, mean_q: 5.229115, mean_eps: 0.978310\n",
      "   526/20000: episode: 26, duration: 0.122s, episode steps:  29, steps per second: 237, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.815409, mae: 3.204941, mean_q: 5.858202, mean_eps: 0.977005\n",
      "   564/20000: episode: 27, duration: 0.155s, episode steps:  38, steps per second: 244, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.682480, mae: 3.262539, mean_q: 6.129884, mean_eps: 0.975498\n",
      "   583/20000: episode: 28, duration: 0.077s, episode steps:  19, steps per second: 246, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.580837, mae: 3.270138, mean_q: 6.242820, mean_eps: 0.974215\n",
      "   625/20000: episode: 29, duration: 0.169s, episode steps:  42, steps per second: 248, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.750490, mae: 3.560182, mean_q: 6.677393, mean_eps: 0.972842\n",
      "   657/20000: episode: 30, duration: 0.128s, episode steps:  32, steps per second: 251, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.469762, mae: 3.724953, mean_q: 7.134161, mean_eps: 0.971178\n",
      "   687/20000: episode: 31, duration: 0.123s, episode steps:  30, steps per second: 244, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.554102, mae: 3.743076, mean_q: 7.241476, mean_eps: 0.969783\n",
      "   698/20000: episode: 32, duration: 0.047s, episode steps:  11, steps per second: 234, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.011032, mae: 3.823819, mean_q: 7.319807, mean_eps: 0.968860\n",
      "   713/20000: episode: 33, duration: 0.065s, episode steps:  15, steps per second: 232, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.677538, mae: 3.963704, mean_q: 7.334140, mean_eps: 0.968275\n",
      "   745/20000: episode: 34, duration: 0.143s, episode steps:  32, steps per second: 223, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.734410, mae: 4.082368, mean_q: 7.933688, mean_eps: 0.967218\n",
      "   848/20000: episode: 35, duration: 0.453s, episode steps: 103, steps per second: 227, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.925233, mae: 4.295273, mean_q: 8.161188, mean_eps: 0.964180\n",
      "   860/20000: episode: 36, duration: 0.055s, episode steps:  12, steps per second: 217, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.933094, mae: 4.509851, mean_q: 8.715165, mean_eps: 0.961592\n",
      "   879/20000: episode: 37, duration: 0.085s, episode steps:  19, steps per second: 223, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.739667, mae: 4.506591, mean_q: 8.691458, mean_eps: 0.960895\n",
      "   890/20000: episode: 38, duration: 0.053s, episode steps:  11, steps per second: 209, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.732105, mae: 4.517212, mean_q: 8.798955, mean_eps: 0.960220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   901/20000: episode: 39, duration: 0.053s, episode steps:  11, steps per second: 206, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.004929, mae: 4.497925, mean_q: 8.615101, mean_eps: 0.959725\n",
      "   917/20000: episode: 40, duration: 0.074s, episode steps:  16, steps per second: 216, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.800137, mae: 4.914974, mean_q: 9.435211, mean_eps: 0.959118\n",
      "   937/20000: episode: 41, duration: 0.087s, episode steps:  20, steps per second: 229, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.725345, mae: 4.865154, mean_q: 9.432931, mean_eps: 0.958307\n",
      "   956/20000: episode: 42, duration: 0.083s, episode steps:  19, steps per second: 230, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.640625, mae: 4.847328, mean_q: 9.597269, mean_eps: 0.957430\n",
      "   975/20000: episode: 43, duration: 0.089s, episode steps:  19, steps per second: 213, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.782096, mae: 4.853052, mean_q: 9.500360, mean_eps: 0.956575\n",
      "   997/20000: episode: 44, duration: 0.110s, episode steps:  22, steps per second: 199, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.058686, mae: 4.829245, mean_q: 9.328729, mean_eps: 0.955652\n",
      "  1016/20000: episode: 45, duration: 0.095s, episode steps:  19, steps per second: 199, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.066401, mae: 5.106621, mean_q: 9.719686, mean_eps: 0.954730\n",
      "  1041/20000: episode: 46, duration: 0.102s, episode steps:  25, steps per second: 246, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.931773, mae: 5.233397, mean_q: 10.236552, mean_eps: 0.953740\n",
      "  1064/20000: episode: 47, duration: 0.098s, episode steps:  23, steps per second: 235, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.040645, mae: 5.214753, mean_q: 10.176910, mean_eps: 0.952660\n",
      "  1084/20000: episode: 48, duration: 0.093s, episode steps:  20, steps per second: 215, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.822917, mae: 5.218031, mean_q: 10.237269, mean_eps: 0.951692\n",
      "  1108/20000: episode: 49, duration: 0.097s, episode steps:  24, steps per second: 248, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.708 [0.000, 1.000],  loss: 0.953823, mae: 5.321351, mean_q: 10.334684, mean_eps: 0.950703\n",
      "  1146/20000: episode: 50, duration: 0.152s, episode steps:  38, steps per second: 250, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.232620, mae: 5.629576, mean_q: 11.029685, mean_eps: 0.949308\n",
      "  1172/20000: episode: 51, duration: 0.105s, episode steps:  26, steps per second: 249, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.168486, mae: 5.636625, mean_q: 10.980103, mean_eps: 0.947867\n",
      "  1228/20000: episode: 52, duration: 0.226s, episode steps:  56, steps per second: 248, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.401037, mae: 5.899612, mean_q: 11.399909, mean_eps: 0.946022\n",
      "  1239/20000: episode: 53, duration: 0.046s, episode steps:  11, steps per second: 237, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.544289, mae: 6.046367, mean_q: 11.653208, mean_eps: 0.944515\n",
      "  1248/20000: episode: 54, duration: 0.040s, episode steps:   9, steps per second: 222, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.074933, mae: 6.115873, mean_q: 12.070963, mean_eps: 0.944065\n",
      "  1266/20000: episode: 55, duration: 0.074s, episode steps:  18, steps per second: 243, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.695403, mae: 6.097097, mean_q: 12.071443, mean_eps: 0.943457\n",
      "  1291/20000: episode: 56, duration: 0.102s, episode steps:  25, steps per second: 245, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.680 [0.000, 1.000],  loss: 1.031896, mae: 6.100065, mean_q: 12.055603, mean_eps: 0.942490\n",
      "  1313/20000: episode: 57, duration: 0.093s, episode steps:  22, steps per second: 236, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.472124, mae: 6.271740, mean_q: 12.216707, mean_eps: 0.941433\n",
      "  1329/20000: episode: 58, duration: 0.067s, episode steps:  16, steps per second: 237, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.441221, mae: 6.525976, mean_q: 13.139769, mean_eps: 0.940578\n",
      "  1343/20000: episode: 59, duration: 0.060s, episode steps:  14, steps per second: 235, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.381219, mae: 6.460391, mean_q: 12.649859, mean_eps: 0.939903\n",
      "  1352/20000: episode: 60, duration: 0.040s, episode steps:   9, steps per second: 224, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.055411, mae: 6.449701, mean_q: 12.860775, mean_eps: 0.939385\n",
      "  1367/20000: episode: 61, duration: 0.064s, episode steps:  15, steps per second: 233, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.844299, mae: 6.529050, mean_q: 13.103640, mean_eps: 0.938845\n",
      "  1380/20000: episode: 62, duration: 0.056s, episode steps:  13, steps per second: 231, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.047424, mae: 6.403408, mean_q: 12.693945, mean_eps: 0.938215\n",
      "  1397/20000: episode: 63, duration: 0.074s, episode steps:  17, steps per second: 230, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.010923, mae: 6.486875, mean_q: 12.985384, mean_eps: 0.937540\n",
      "  1406/20000: episode: 64, duration: 0.043s, episode steps:   9, steps per second: 208, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.474469, mae: 6.478754, mean_q: 12.496611, mean_eps: 0.936955\n",
      "  1421/20000: episode: 65, duration: 0.066s, episode steps:  15, steps per second: 227, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.285515, mae: 6.826760, mean_q: 13.586730, mean_eps: 0.936415\n",
      "  1441/20000: episode: 66, duration: 0.084s, episode steps:  20, steps per second: 239, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.940236, mae: 6.691723, mean_q: 13.411560, mean_eps: 0.935628\n",
      "  1527/20000: episode: 67, duration: 0.387s, episode steps:  86, steps per second: 222, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.115351, mae: 6.920441, mean_q: 13.788935, mean_eps: 0.933243\n",
      "  1544/20000: episode: 68, duration: 0.073s, episode steps:  17, steps per second: 234, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.924806, mae: 7.201428, mean_q: 14.148214, mean_eps: 0.930925\n",
      "  1596/20000: episode: 69, duration: 0.211s, episode steps:  52, steps per second: 246, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.096741, mae: 7.310980, mean_q: 14.631179, mean_eps: 0.929372\n",
      "  1619/20000: episode: 70, duration: 0.095s, episode steps:  23, steps per second: 242, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 1.447110, mae: 7.774674, mean_q: 15.315526, mean_eps: 0.927685\n",
      "  1641/20000: episode: 71, duration: 0.090s, episode steps:  22, steps per second: 244, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.056302, mae: 7.772635, mean_q: 15.305139, mean_eps: 0.926673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1683/20000: episode: 72, duration: 0.172s, episode steps:  42, steps per second: 244, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.381814, mae: 7.792979, mean_q: 15.535343, mean_eps: 0.925232\n",
      "  1704/20000: episode: 73, duration: 0.095s, episode steps:  21, steps per second: 220, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.600930, mae: 7.763653, mean_q: 15.442273, mean_eps: 0.923815\n",
      "  1714/20000: episode: 74, duration: 0.048s, episode steps:  10, steps per second: 210, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.793180, mae: 8.256749, mean_q: 16.455031, mean_eps: 0.923118\n",
      "  1728/20000: episode: 75, duration: 0.064s, episode steps:  14, steps per second: 220, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.546671, mae: 8.264487, mean_q: 16.462335, mean_eps: 0.922578\n",
      "  1800/20000: episode: 76, duration: 0.297s, episode steps:  72, steps per second: 242, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.446080, mae: 8.286598, mean_q: 16.534395, mean_eps: 0.920643\n",
      "  1820/20000: episode: 77, duration: 0.085s, episode steps:  20, steps per second: 235, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.786978, mae: 8.664973, mean_q: 17.150045, mean_eps: 0.918572\n",
      "  1839/20000: episode: 78, duration: 0.088s, episode steps:  19, steps per second: 217, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.666388, mae: 8.809946, mean_q: 17.707916, mean_eps: 0.917695\n",
      "  1908/20000: episode: 79, duration: 0.282s, episode steps:  69, steps per second: 245, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 1.874374, mae: 8.800673, mean_q: 17.534494, mean_eps: 0.915715\n",
      "  1925/20000: episode: 80, duration: 0.076s, episode steps:  17, steps per second: 223, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.735018, mae: 9.344880, mean_q: 18.639296, mean_eps: 0.913780\n",
      "  1960/20000: episode: 81, duration: 0.138s, episode steps:  35, steps per second: 253, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.105818, mae: 9.237193, mean_q: 18.350825, mean_eps: 0.912610\n",
      "  1980/20000: episode: 82, duration: 0.082s, episode steps:  20, steps per second: 243, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 1.622882, mae: 9.307995, mean_q: 18.600514, mean_eps: 0.911373\n",
      "  2012/20000: episode: 83, duration: 0.138s, episode steps:  32, steps per second: 231, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.134401, mae: 9.386329, mean_q: 18.612968, mean_eps: 0.910203\n",
      "  2043/20000: episode: 84, duration: 0.189s, episode steps:  31, steps per second: 164, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.560533, mae: 9.766292, mean_q: 19.607843, mean_eps: 0.908785\n",
      "  2060/20000: episode: 85, duration: 0.103s, episode steps:  17, steps per second: 165, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 2.005243, mae: 9.740008, mean_q: 19.552977, mean_eps: 0.907705\n",
      "  2078/20000: episode: 86, duration: 0.074s, episode steps:  18, steps per second: 245, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.861104, mae: 9.882754, mean_q: 19.882709, mean_eps: 0.906918\n",
      "  2094/20000: episode: 87, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.548353, mae: 9.866416, mean_q: 19.872415, mean_eps: 0.906152\n",
      "  2120/20000: episode: 88, duration: 0.108s, episode steps:  26, steps per second: 240, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.505427, mae: 10.056879, mean_q: 20.055532, mean_eps: 0.905207\n",
      "  2140/20000: episode: 89, duration: 0.079s, episode steps:  20, steps per second: 253, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.722427, mae: 10.163889, mean_q: 20.216678, mean_eps: 0.904173\n",
      "  2180/20000: episode: 90, duration: 0.173s, episode steps:  40, steps per second: 232, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.084480, mae: 10.297255, mean_q: 20.615072, mean_eps: 0.902823\n",
      "  2192/20000: episode: 91, duration: 0.060s, episode steps:  12, steps per second: 199, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.918144, mae: 10.326131, mean_q: 20.590734, mean_eps: 0.901653\n",
      "  2210/20000: episode: 92, duration: 0.092s, episode steps:  18, steps per second: 195, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.544104, mae: 10.418181, mean_q: 20.766391, mean_eps: 0.900977\n",
      "  2228/20000: episode: 93, duration: 0.092s, episode steps:  18, steps per second: 196, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.405853, mae: 10.870169, mean_q: 21.944720, mean_eps: 0.900168\n",
      "  2279/20000: episode: 94, duration: 0.216s, episode steps:  51, steps per second: 237, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.231609, mae: 10.725358, mean_q: 21.586037, mean_eps: 0.898615\n",
      "  2337/20000: episode: 95, duration: 0.233s, episode steps:  58, steps per second: 249, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.140400, mae: 11.056221, mean_q: 22.239834, mean_eps: 0.896163\n",
      "  2366/20000: episode: 96, duration: 0.115s, episode steps:  29, steps per second: 253, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.924206, mae: 11.247907, mean_q: 22.678972, mean_eps: 0.894205\n",
      "  2379/20000: episode: 97, duration: 0.053s, episode steps:  13, steps per second: 245, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.859217, mae: 11.160398, mean_q: 22.609724, mean_eps: 0.893260\n",
      "  2394/20000: episode: 98, duration: 0.066s, episode steps:  15, steps per second: 226, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.426098, mae: 11.155516, mean_q: 22.659922, mean_eps: 0.892630\n",
      "  2432/20000: episode: 99, duration: 0.151s, episode steps:  38, steps per second: 252, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 2.264701, mae: 11.710124, mean_q: 23.568946, mean_eps: 0.891437\n",
      "  2453/20000: episode: 100, duration: 0.089s, episode steps:  21, steps per second: 237, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 1.988155, mae: 11.913528, mean_q: 24.090478, mean_eps: 0.890110\n",
      "  2475/20000: episode: 101, duration: 0.089s, episode steps:  22, steps per second: 246, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.360333, mae: 11.647373, mean_q: 23.510974, mean_eps: 0.889143\n",
      "  2497/20000: episode: 102, duration: 0.089s, episode steps:  22, steps per second: 248, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.301258, mae: 11.929878, mean_q: 24.059115, mean_eps: 0.888153\n",
      "  2513/20000: episode: 103, duration: 0.070s, episode steps:  16, steps per second: 228, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.708020, mae: 12.071559, mean_q: 24.164316, mean_eps: 0.887298\n",
      "  2530/20000: episode: 104, duration: 0.071s, episode steps:  17, steps per second: 240, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.812261, mae: 12.251302, mean_q: 24.809464, mean_eps: 0.886555\n",
      "  2556/20000: episode: 105, duration: 0.134s, episode steps:  26, steps per second: 194, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.339839, mae: 12.165698, mean_q: 24.517080, mean_eps: 0.885587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2576/20000: episode: 106, duration: 0.093s, episode steps:  20, steps per second: 214, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 1.751593, mae: 12.157021, mean_q: 24.764588, mean_eps: 0.884553\n",
      "  2601/20000: episode: 107, duration: 0.101s, episode steps:  25, steps per second: 248, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.142488, mae: 12.212011, mean_q: 24.729618, mean_eps: 0.883540\n",
      "  2609/20000: episode: 108, duration: 0.034s, episode steps:   8, steps per second: 233, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.329339, mae: 12.618757, mean_q: 25.432443, mean_eps: 0.882797\n",
      "  2710/20000: episode: 109, duration: 0.392s, episode steps: 101, steps per second: 257, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.048671, mae: 12.685647, mean_q: 25.699464, mean_eps: 0.880345\n",
      "  2722/20000: episode: 110, duration: 0.049s, episode steps:  12, steps per second: 244, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.501830, mae: 13.278089, mean_q: 26.932095, mean_eps: 0.877802\n",
      "  2748/20000: episode: 111, duration: 0.107s, episode steps:  26, steps per second: 243, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.693401, mae: 12.887262, mean_q: 26.264490, mean_eps: 0.876947\n",
      "  2761/20000: episode: 112, duration: 0.062s, episode steps:  13, steps per second: 211, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 3.084870, mae: 12.960579, mean_q: 26.194074, mean_eps: 0.876070\n",
      "  2774/20000: episode: 113, duration: 0.056s, episode steps:  13, steps per second: 231, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 2.066738, mae: 13.134734, mean_q: 26.659791, mean_eps: 0.875485\n",
      "  2791/20000: episode: 114, duration: 0.072s, episode steps:  17, steps per second: 236, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 3.023642, mae: 13.275473, mean_q: 26.650627, mean_eps: 0.874810\n",
      "  2804/20000: episode: 115, duration: 0.057s, episode steps:  13, steps per second: 227, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.757851, mae: 13.175927, mean_q: 26.612734, mean_eps: 0.874135\n",
      "  2821/20000: episode: 116, duration: 0.076s, episode steps:  17, steps per second: 223, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.370605, mae: 13.962738, mean_q: 28.398372, mean_eps: 0.873460\n",
      "  2841/20000: episode: 117, duration: 0.084s, episode steps:  20, steps per second: 238, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.276297, mae: 13.304948, mean_q: 27.050833, mean_eps: 0.872627\n",
      "  2852/20000: episode: 118, duration: 0.047s, episode steps:  11, steps per second: 233, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.436147, mae: 13.705550, mean_q: 27.727994, mean_eps: 0.871930\n",
      "  2865/20000: episode: 119, duration: 0.056s, episode steps:  13, steps per second: 233, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.420946, mae: 13.763975, mean_q: 27.912692, mean_eps: 0.871390\n",
      "  2880/20000: episode: 120, duration: 0.064s, episode steps:  15, steps per second: 235, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 1.333468, mae: 13.527518, mean_q: 27.717980, mean_eps: 0.870760\n",
      "  2895/20000: episode: 121, duration: 0.061s, episode steps:  15, steps per second: 245, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.609762, mae: 13.518301, mean_q: 27.534077, mean_eps: 0.870085\n",
      "  2915/20000: episode: 122, duration: 0.081s, episode steps:  20, steps per second: 248, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 2.075297, mae: 13.890010, mean_q: 28.243209, mean_eps: 0.869298\n",
      "  2952/20000: episode: 123, duration: 0.148s, episode steps:  37, steps per second: 251, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.016560, mae: 14.146986, mean_q: 28.669260, mean_eps: 0.868015\n",
      "  2992/20000: episode: 124, duration: 0.156s, episode steps:  40, steps per second: 256, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.157208, mae: 14.063492, mean_q: 28.729481, mean_eps: 0.866282\n",
      "  3031/20000: episode: 125, duration: 0.158s, episode steps:  39, steps per second: 247, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.641 [0.000, 1.000],  loss: 3.028881, mae: 14.428008, mean_q: 29.243398, mean_eps: 0.864505\n",
      "  3054/20000: episode: 126, duration: 0.092s, episode steps:  23, steps per second: 249, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 2.095745, mae: 14.442178, mean_q: 29.548659, mean_eps: 0.863110\n",
      "  3079/20000: episode: 127, duration: 0.117s, episode steps:  25, steps per second: 213, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.152812, mae: 14.626910, mean_q: 30.037584, mean_eps: 0.862030\n",
      "  3096/20000: episode: 128, duration: 0.086s, episode steps:  17, steps per second: 197, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.234101, mae: 14.599100, mean_q: 29.792209, mean_eps: 0.861085\n",
      "  3111/20000: episode: 129, duration: 0.068s, episode steps:  15, steps per second: 220, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.127859, mae: 14.361373, mean_q: 29.296653, mean_eps: 0.860365\n",
      "  3124/20000: episode: 130, duration: 0.056s, episode steps:  13, steps per second: 232, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.420171, mae: 15.179469, mean_q: 30.905865, mean_eps: 0.859735\n",
      "  3151/20000: episode: 131, duration: 0.113s, episode steps:  27, steps per second: 240, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.978664, mae: 14.847242, mean_q: 30.488842, mean_eps: 0.858835\n",
      "  3183/20000: episode: 132, duration: 0.128s, episode steps:  32, steps per second: 251, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.368301, mae: 15.103126, mean_q: 30.831919, mean_eps: 0.857507\n",
      "  3208/20000: episode: 133, duration: 0.101s, episode steps:  25, steps per second: 247, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 2.600835, mae: 15.169714, mean_q: 30.988493, mean_eps: 0.856225\n",
      "  3250/20000: episode: 134, duration: 0.184s, episode steps:  42, steps per second: 228, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.722362, mae: 15.573576, mean_q: 31.849252, mean_eps: 0.854717\n",
      "  3283/20000: episode: 135, duration: 0.133s, episode steps:  33, steps per second: 249, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 2.303919, mae: 15.427601, mean_q: 31.460712, mean_eps: 0.853030\n",
      "  3303/20000: episode: 136, duration: 0.081s, episode steps:  20, steps per second: 246, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.170776, mae: 15.743410, mean_q: 32.148124, mean_eps: 0.851838\n",
      "  3330/20000: episode: 137, duration: 0.106s, episode steps:  27, steps per second: 255, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.241978, mae: 16.352669, mean_q: 33.386103, mean_eps: 0.850780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3347/20000: episode: 138, duration: 0.074s, episode steps:  17, steps per second: 231, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 3.091754, mae: 16.213171, mean_q: 33.101388, mean_eps: 0.849790\n",
      "  3369/20000: episode: 139, duration: 0.090s, episode steps:  22, steps per second: 244, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.496364, mae: 16.017555, mean_q: 32.688542, mean_eps: 0.848913\n",
      "  3391/20000: episode: 140, duration: 0.089s, episode steps:  22, steps per second: 247, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.975124, mae: 16.278721, mean_q: 33.149899, mean_eps: 0.847922\n",
      "  3431/20000: episode: 141, duration: 0.168s, episode steps:  40, steps per second: 238, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.726358, mae: 16.656161, mean_q: 34.118815, mean_eps: 0.846527\n",
      "  3440/20000: episode: 142, duration: 0.049s, episode steps:   9, steps per second: 185, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 4.342026, mae: 17.053520, mean_q: 34.536897, mean_eps: 0.845425\n",
      "  3490/20000: episode: 143, duration: 0.221s, episode steps:  50, steps per second: 226, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.371625, mae: 16.973001, mean_q: 34.465735, mean_eps: 0.844098\n",
      "  3509/20000: episode: 144, duration: 0.088s, episode steps:  19, steps per second: 217, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 3.804670, mae: 16.894253, mean_q: 34.231347, mean_eps: 0.842545\n",
      "  3530/20000: episode: 145, duration: 0.101s, episode steps:  21, steps per second: 209, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.478617, mae: 17.108870, mean_q: 34.536607, mean_eps: 0.841645\n",
      "  3546/20000: episode: 146, duration: 0.076s, episode steps:  16, steps per second: 210, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.670618, mae: 17.372988, mean_q: 35.177459, mean_eps: 0.840812\n",
      "  3557/20000: episode: 147, duration: 0.054s, episode steps:  11, steps per second: 203, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.247939, mae: 17.064902, mean_q: 34.795553, mean_eps: 0.840205\n",
      "  3583/20000: episode: 148, duration: 0.131s, episode steps:  26, steps per second: 199, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.177061, mae: 16.856888, mean_q: 34.429730, mean_eps: 0.839372\n",
      "  3648/20000: episode: 149, duration: 0.329s, episode steps:  65, steps per second: 198, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 2.579234, mae: 17.562590, mean_q: 35.899794, mean_eps: 0.837325\n",
      "  3665/20000: episode: 150, duration: 0.079s, episode steps:  17, steps per second: 216, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.037404, mae: 17.796756, mean_q: 36.471504, mean_eps: 0.835480\n",
      "  3741/20000: episode: 151, duration: 0.359s, episode steps:  76, steps per second: 212, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.194829, mae: 17.813262, mean_q: 36.400698, mean_eps: 0.833387\n",
      "  3753/20000: episode: 152, duration: 0.058s, episode steps:  12, steps per second: 206, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 3.271908, mae: 18.143936, mean_q: 36.873224, mean_eps: 0.831408\n",
      "  3774/20000: episode: 153, duration: 0.097s, episode steps:  21, steps per second: 216, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.349956, mae: 18.267448, mean_q: 37.539330, mean_eps: 0.830665\n",
      "  3792/20000: episode: 154, duration: 0.084s, episode steps:  18, steps per second: 215, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.187732, mae: 18.346186, mean_q: 37.785371, mean_eps: 0.829787\n",
      "  3835/20000: episode: 155, duration: 0.192s, episode steps:  43, steps per second: 224, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.560501, mae: 18.646388, mean_q: 38.280403, mean_eps: 0.828415\n",
      "  3963/20000: episode: 156, duration: 0.553s, episode steps: 128, steps per second: 232, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.584232, mae: 18.602710, mean_q: 38.080356, mean_eps: 0.824567\n",
      "  4011/20000: episode: 157, duration: 0.195s, episode steps:  48, steps per second: 247, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.123814, mae: 19.031800, mean_q: 39.025626, mean_eps: 0.820607\n",
      "  4039/20000: episode: 158, duration: 0.116s, episode steps:  28, steps per second: 241, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.269776, mae: 19.894562, mean_q: 40.475802, mean_eps: 0.818898\n",
      "  4086/20000: episode: 159, duration: 0.201s, episode steps:  47, steps per second: 233, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 3.470475, mae: 19.427582, mean_q: 39.876052, mean_eps: 0.817210\n",
      "  4200/20000: episode: 160, duration: 0.472s, episode steps: 114, steps per second: 241, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.764305, mae: 19.828423, mean_q: 40.791545, mean_eps: 0.813588\n",
      "  4233/20000: episode: 161, duration: 0.137s, episode steps:  33, steps per second: 241, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.141158, mae: 20.674059, mean_q: 42.379678, mean_eps: 0.810280\n",
      "  4241/20000: episode: 162, duration: 0.035s, episode steps:   8, steps per second: 231, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 3.314870, mae: 20.776694, mean_q: 42.782365, mean_eps: 0.809357\n",
      "  4264/20000: episode: 163, duration: 0.096s, episode steps:  23, steps per second: 239, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.818059, mae: 20.381286, mean_q: 41.942409, mean_eps: 0.808660\n",
      "  4338/20000: episode: 164, duration: 0.310s, episode steps:  74, steps per second: 238, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.400898, mae: 20.699669, mean_q: 42.534299, mean_eps: 0.806477\n",
      "  4371/20000: episode: 165, duration: 0.139s, episode steps:  33, steps per second: 238, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 4.366595, mae: 20.816682, mean_q: 42.716529, mean_eps: 0.804070\n",
      "  4428/20000: episode: 166, duration: 0.237s, episode steps:  57, steps per second: 240, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 3.036480, mae: 21.140252, mean_q: 43.412636, mean_eps: 0.802045\n",
      "  4465/20000: episode: 167, duration: 0.154s, episode steps:  37, steps per second: 241, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 6.110995, mae: 21.526504, mean_q: 44.013922, mean_eps: 0.799930\n",
      "  4479/20000: episode: 168, duration: 0.068s, episode steps:  14, steps per second: 206, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 4.061609, mae: 21.482525, mean_q: 44.245559, mean_eps: 0.798782\n",
      "  4504/20000: episode: 169, duration: 0.108s, episode steps:  25, steps per second: 231, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.929192, mae: 21.381389, mean_q: 43.689040, mean_eps: 0.797905\n",
      "  4522/20000: episode: 170, duration: 0.073s, episode steps:  18, steps per second: 246, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.740172, mae: 22.185717, mean_q: 45.531703, mean_eps: 0.796937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4569/20000: episode: 171, duration: 0.209s, episode steps:  47, steps per second: 224, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.194921, mae: 21.954928, mean_q: 44.935714, mean_eps: 0.795475\n",
      "  4613/20000: episode: 172, duration: 0.243s, episode steps:  44, steps per second: 181, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.828725, mae: 21.836103, mean_q: 44.885540, mean_eps: 0.793427\n",
      "  4623/20000: episode: 173, duration: 0.054s, episode steps:  10, steps per second: 185, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.603038, mae: 23.293102, mean_q: 47.496779, mean_eps: 0.792212\n",
      "  4753/20000: episode: 174, duration: 0.595s, episode steps: 130, steps per second: 218, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.422066, mae: 22.955472, mean_q: 47.251763, mean_eps: 0.789062\n",
      "  4803/20000: episode: 175, duration: 0.210s, episode steps:  50, steps per second: 238, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.261101, mae: 22.987899, mean_q: 47.400017, mean_eps: 0.785013\n",
      "  4909/20000: episode: 176, duration: 0.427s, episode steps: 106, steps per second: 248, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.618106, mae: 24.050462, mean_q: 49.525279, mean_eps: 0.781502\n",
      "  4944/20000: episode: 177, duration: 0.142s, episode steps:  35, steps per second: 247, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.847333, mae: 24.603685, mean_q: 50.713294, mean_eps: 0.778330\n",
      "  4979/20000: episode: 178, duration: 0.142s, episode steps:  35, steps per second: 247, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.259688, mae: 24.455344, mean_q: 50.186869, mean_eps: 0.776755\n",
      "  5016/20000: episode: 179, duration: 0.148s, episode steps:  37, steps per second: 250, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 4.176127, mae: 24.779495, mean_q: 50.853819, mean_eps: 0.775135\n",
      "  5054/20000: episode: 180, duration: 0.156s, episode steps:  38, steps per second: 244, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.542150, mae: 24.775981, mean_q: 50.983137, mean_eps: 0.773447\n",
      "  5076/20000: episode: 181, duration: 0.089s, episode steps:  22, steps per second: 248, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.968988, mae: 25.077432, mean_q: 51.523709, mean_eps: 0.772097\n",
      "  5118/20000: episode: 182, duration: 0.185s, episode steps:  42, steps per second: 227, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.965762, mae: 25.376209, mean_q: 52.309131, mean_eps: 0.770657\n",
      "  5145/20000: episode: 183, duration: 0.126s, episode steps:  27, steps per second: 215, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 3.430052, mae: 25.622441, mean_q: 53.038460, mean_eps: 0.769105\n",
      "  5166/20000: episode: 184, duration: 0.088s, episode steps:  21, steps per second: 240, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5.297420, mae: 25.857656, mean_q: 53.041400, mean_eps: 0.768025\n",
      "  5217/20000: episode: 185, duration: 0.204s, episode steps:  51, steps per second: 249, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.267431, mae: 25.885885, mean_q: 53.347757, mean_eps: 0.766405\n",
      "  5233/20000: episode: 186, duration: 0.070s, episode steps:  16, steps per second: 230, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 9.706311, mae: 27.008214, mean_q: 55.037406, mean_eps: 0.764898\n",
      "  5328/20000: episode: 187, duration: 0.377s, episode steps:  95, steps per second: 252, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.575192, mae: 26.822974, mean_q: 54.857915, mean_eps: 0.762400\n",
      "  5340/20000: episode: 188, duration: 0.050s, episode steps:  12, steps per second: 239, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 6.569297, mae: 27.353054, mean_q: 56.099736, mean_eps: 0.759993\n",
      "  5355/20000: episode: 189, duration: 0.062s, episode steps:  15, steps per second: 242, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.753820, mae: 27.663385, mean_q: 56.814798, mean_eps: 0.759385\n",
      "  5376/20000: episode: 190, duration: 0.086s, episode steps:  21, steps per second: 245, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.487987, mae: 26.711271, mean_q: 55.425942, mean_eps: 0.758575\n",
      "  5438/20000: episode: 191, duration: 0.251s, episode steps:  62, steps per second: 247, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 9.427360, mae: 27.057734, mean_q: 55.380975, mean_eps: 0.756708\n",
      "  5459/20000: episode: 192, duration: 0.085s, episode steps:  21, steps per second: 246, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.182917, mae: 26.945267, mean_q: 55.380447, mean_eps: 0.754840\n",
      "  5523/20000: episode: 193, duration: 0.255s, episode steps:  64, steps per second: 251, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.137518, mae: 27.586254, mean_q: 56.393109, mean_eps: 0.752927\n",
      "  5540/20000: episode: 194, duration: 0.071s, episode steps:  17, steps per second: 240, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 5.032147, mae: 27.993203, mean_q: 57.357229, mean_eps: 0.751105\n",
      "  5554/20000: episode: 195, duration: 0.058s, episode steps:  14, steps per second: 243, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 15.588214, mae: 28.387576, mean_q: 57.729929, mean_eps: 0.750407\n",
      "  5592/20000: episode: 196, duration: 0.155s, episode steps:  38, steps per second: 246, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.277820, mae: 27.874715, mean_q: 56.947218, mean_eps: 0.749237\n",
      "  5626/20000: episode: 197, duration: 0.136s, episode steps:  34, steps per second: 250, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.785309, mae: 28.122114, mean_q: 57.554416, mean_eps: 0.747617\n",
      "  5661/20000: episode: 198, duration: 0.164s, episode steps:  35, steps per second: 213, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 9.530493, mae: 28.407799, mean_q: 58.391375, mean_eps: 0.746065\n",
      "  5699/20000: episode: 199, duration: 0.179s, episode steps:  38, steps per second: 212, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9.043214, mae: 28.472277, mean_q: 58.410869, mean_eps: 0.744423\n",
      "  5731/20000: episode: 200, duration: 0.149s, episode steps:  32, steps per second: 215, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.012120, mae: 28.927064, mean_q: 59.233755, mean_eps: 0.742847\n",
      "  5747/20000: episode: 201, duration: 0.076s, episode steps:  16, steps per second: 211, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 7.106424, mae: 29.362758, mean_q: 60.018434, mean_eps: 0.741767\n",
      "  5763/20000: episode: 202, duration: 0.071s, episode steps:  16, steps per second: 224, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 8.099051, mae: 29.433054, mean_q: 60.055195, mean_eps: 0.741048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5802/20000: episode: 203, duration: 0.168s, episode steps:  39, steps per second: 232, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 7.463361, mae: 29.229952, mean_q: 59.740000, mean_eps: 0.739810\n",
      "  5885/20000: episode: 204, duration: 0.335s, episode steps:  83, steps per second: 248, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 6.795105, mae: 29.842550, mean_q: 61.047137, mean_eps: 0.737065\n",
      "  5903/20000: episode: 205, duration: 0.076s, episode steps:  18, steps per second: 238, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 4.114629, mae: 30.551089, mean_q: 62.218828, mean_eps: 0.734793\n",
      "  5930/20000: episode: 206, duration: 0.108s, episode steps:  27, steps per second: 249, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.771493, mae: 29.612103, mean_q: 60.928421, mean_eps: 0.733780\n",
      "  5969/20000: episode: 207, duration: 0.168s, episode steps:  39, steps per second: 232, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 6.853101, mae: 29.970046, mean_q: 61.690673, mean_eps: 0.732295\n",
      "  5982/20000: episode: 208, duration: 0.057s, episode steps:  13, steps per second: 229, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.930989, mae: 30.057280, mean_q: 61.920164, mean_eps: 0.731125\n",
      "  6013/20000: episode: 209, duration: 0.127s, episode steps:  31, steps per second: 244, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.720756, mae: 30.416729, mean_q: 62.661013, mean_eps: 0.730135\n",
      "  6040/20000: episode: 210, duration: 0.114s, episode steps:  27, steps per second: 237, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 9.848329, mae: 30.481668, mean_q: 62.783687, mean_eps: 0.728830\n",
      "  6097/20000: episode: 211, duration: 0.259s, episode steps:  57, steps per second: 220, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 9.382092, mae: 30.820912, mean_q: 63.280648, mean_eps: 0.726940\n",
      "  6110/20000: episode: 212, duration: 0.055s, episode steps:  13, steps per second: 236, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 7.960262, mae: 30.709170, mean_q: 62.876076, mean_eps: 0.725365\n",
      "  6144/20000: episode: 213, duration: 0.137s, episode steps:  34, steps per second: 249, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 11.667775, mae: 31.237518, mean_q: 63.911797, mean_eps: 0.724307\n",
      "  6155/20000: episode: 214, duration: 0.056s, episode steps:  11, steps per second: 196, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 8.740286, mae: 31.497032, mean_q: 64.192302, mean_eps: 0.723295\n",
      "  6250/20000: episode: 215, duration: 0.405s, episode steps:  95, steps per second: 235, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 9.465476, mae: 31.431761, mean_q: 64.397186, mean_eps: 0.720910\n",
      "  6266/20000: episode: 216, duration: 0.066s, episode steps:  16, steps per second: 241, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 9.202696, mae: 32.548905, mean_q: 66.936792, mean_eps: 0.718412\n",
      "  6283/20000: episode: 217, duration: 0.070s, episode steps:  17, steps per second: 241, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 9.179499, mae: 31.837615, mean_q: 65.301507, mean_eps: 0.717670\n",
      "  6339/20000: episode: 218, duration: 0.231s, episode steps:  56, steps per second: 243, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 10.286205, mae: 32.074210, mean_q: 65.589362, mean_eps: 0.716028\n",
      "  6378/20000: episode: 219, duration: 0.156s, episode steps:  39, steps per second: 251, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 10.224718, mae: 32.517692, mean_q: 66.230560, mean_eps: 0.713890\n",
      "  6435/20000: episode: 220, duration: 0.230s, episode steps:  57, steps per second: 247, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 7.687452, mae: 32.999797, mean_q: 67.638370, mean_eps: 0.711730\n",
      "  6465/20000: episode: 221, duration: 0.120s, episode steps:  30, steps per second: 250, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.205490, mae: 32.498516, mean_q: 66.651639, mean_eps: 0.709772\n",
      "  6480/20000: episode: 222, duration: 0.061s, episode steps:  15, steps per second: 246, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 13.205409, mae: 32.811729, mean_q: 67.002486, mean_eps: 0.708760\n",
      "  6494/20000: episode: 223, duration: 0.061s, episode steps:  14, steps per second: 231, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 7.752749, mae: 33.085977, mean_q: 67.775140, mean_eps: 0.708108\n",
      "  6562/20000: episode: 224, duration: 0.278s, episode steps:  68, steps per second: 244, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.184406, mae: 33.428272, mean_q: 68.475191, mean_eps: 0.706262\n",
      "  6631/20000: episode: 225, duration: 0.285s, episode steps:  69, steps per second: 242, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8.450948, mae: 33.396630, mean_q: 68.744049, mean_eps: 0.703180\n",
      "  6702/20000: episode: 226, duration: 0.299s, episode steps:  71, steps per second: 238, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 8.957083, mae: 33.559899, mean_q: 69.185038, mean_eps: 0.700030\n",
      "  6724/20000: episode: 227, duration: 0.103s, episode steps:  22, steps per second: 214, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8.242564, mae: 34.664616, mean_q: 71.221489, mean_eps: 0.697937\n",
      "  6748/20000: episode: 228, duration: 0.095s, episode steps:  24, steps per second: 253, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 7.576271, mae: 34.429580, mean_q: 70.954629, mean_eps: 0.696903\n",
      "  6782/20000: episode: 229, duration: 0.139s, episode steps:  34, steps per second: 245, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.495125, mae: 34.716527, mean_q: 71.357948, mean_eps: 0.695598\n",
      "  6797/20000: episode: 230, duration: 0.062s, episode steps:  15, steps per second: 243, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 11.313607, mae: 34.212366, mean_q: 70.090201, mean_eps: 0.694495\n",
      "  6872/20000: episode: 231, duration: 0.308s, episode steps:  75, steps per second: 244, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 8.144221, mae: 35.488894, mean_q: 72.800047, mean_eps: 0.692470\n",
      "  6945/20000: episode: 232, duration: 0.305s, episode steps:  73, steps per second: 239, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 8.922809, mae: 36.131456, mean_q: 74.035041, mean_eps: 0.689140\n",
      "  6981/20000: episode: 233, duration: 0.143s, episode steps:  36, steps per second: 251, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 9.511921, mae: 36.415479, mean_q: 74.865015, mean_eps: 0.686688\n",
      "  7023/20000: episode: 234, duration: 0.172s, episode steps:  42, steps per second: 244, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 11.058979, mae: 36.682455, mean_q: 75.387047, mean_eps: 0.684933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7133/20000: episode: 235, duration: 0.457s, episode steps: 110, steps per second: 241, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 10.455564, mae: 37.473808, mean_q: 76.677526, mean_eps: 0.681512\n",
      "  7165/20000: episode: 236, duration: 0.134s, episode steps:  32, steps per second: 240, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 13.412396, mae: 38.279504, mean_q: 78.181486, mean_eps: 0.678317\n",
      "  7185/20000: episode: 237, duration: 0.114s, episode steps:  20, steps per second: 176, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.558411, mae: 37.735992, mean_q: 77.040277, mean_eps: 0.677147\n",
      "  7276/20000: episode: 238, duration: 0.500s, episode steps:  91, steps per second: 182, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 8.183508, mae: 38.419084, mean_q: 78.730599, mean_eps: 0.674650\n",
      "  7345/20000: episode: 239, duration: 0.344s, episode steps:  69, steps per second: 201, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.420 [0.000, 1.000],  loss: 10.020997, mae: 39.343928, mean_q: 80.591600, mean_eps: 0.671050\n",
      "  7452/20000: episode: 240, duration: 0.441s, episode steps: 107, steps per second: 242, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.709878, mae: 39.589320, mean_q: 81.065735, mean_eps: 0.667090\n",
      "  7553/20000: episode: 241, duration: 0.403s, episode steps: 101, steps per second: 251, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 13.285970, mae: 39.865686, mean_q: 81.807143, mean_eps: 0.662410\n",
      "  7592/20000: episode: 242, duration: 0.156s, episode steps:  39, steps per second: 249, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 11.671127, mae: 40.682917, mean_q: 83.862601, mean_eps: 0.659260\n",
      "  7619/20000: episode: 243, duration: 0.113s, episode steps:  27, steps per second: 239, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 12.906506, mae: 40.448014, mean_q: 82.853572, mean_eps: 0.657775\n",
      "  7683/20000: episode: 244, duration: 0.280s, episode steps:  64, steps per second: 228, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 10.971205, mae: 40.909085, mean_q: 83.915636, mean_eps: 0.655728\n",
      "  7746/20000: episode: 245, duration: 0.333s, episode steps:  63, steps per second: 189, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 9.956642, mae: 41.516115, mean_q: 85.090465, mean_eps: 0.652870\n",
      "  7798/20000: episode: 246, duration: 0.227s, episode steps:  52, steps per second: 229, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 11.654447, mae: 41.543755, mean_q: 85.302190, mean_eps: 0.650283\n",
      "  7899/20000: episode: 247, duration: 0.438s, episode steps: 101, steps per second: 231, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 11.942832, mae: 42.102354, mean_q: 86.453574, mean_eps: 0.646840\n",
      "  7973/20000: episode: 248, duration: 0.308s, episode steps:  74, steps per second: 240, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 10.712559, mae: 42.758344, mean_q: 87.866140, mean_eps: 0.642903\n",
      "  8029/20000: episode: 249, duration: 0.237s, episode steps:  56, steps per second: 236, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 15.058410, mae: 43.218337, mean_q: 88.776565, mean_eps: 0.639977\n",
      "  8045/20000: episode: 250, duration: 0.067s, episode steps:  16, steps per second: 237, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 5.948944, mae: 43.186512, mean_q: 88.895932, mean_eps: 0.638357\n",
      "  8165/20000: episode: 251, duration: 0.483s, episode steps: 120, steps per second: 249, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 12.255013, mae: 43.621553, mean_q: 89.768730, mean_eps: 0.635297\n",
      "  8234/20000: episode: 252, duration: 0.281s, episode steps:  69, steps per second: 245, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 12.664473, mae: 44.346122, mean_q: 90.709397, mean_eps: 0.631045\n",
      "  8321/20000: episode: 253, duration: 0.390s, episode steps:  87, steps per second: 223, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 14.819843, mae: 44.917478, mean_q: 92.121453, mean_eps: 0.627535\n",
      "  8393/20000: episode: 254, duration: 0.288s, episode steps:  72, steps per second: 250, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 12.370334, mae: 44.786327, mean_q: 91.764118, mean_eps: 0.623957\n",
      "  8420/20000: episode: 255, duration: 0.112s, episode steps:  27, steps per second: 241, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 12.208109, mae: 45.907383, mean_q: 93.893192, mean_eps: 0.621730\n",
      "  8531/20000: episode: 256, duration: 0.456s, episode steps: 111, steps per second: 244, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 15.944501, mae: 45.947101, mean_q: 93.880950, mean_eps: 0.618625\n",
      "  8611/20000: episode: 257, duration: 0.319s, episode steps:  80, steps per second: 251, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 14.121793, mae: 46.189809, mean_q: 95.027984, mean_eps: 0.614327\n",
      "  8718/20000: episode: 258, duration: 0.423s, episode steps: 107, steps per second: 253, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 13.953825, mae: 46.462535, mean_q: 95.196362, mean_eps: 0.610120\n",
      "  8880/20000: episode: 259, duration: 0.689s, episode steps: 162, steps per second: 235, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 16.679851, mae: 47.484973, mean_q: 97.087407, mean_eps: 0.604067\n",
      "  8930/20000: episode: 260, duration: 0.206s, episode steps:  50, steps per second: 243, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 15.105593, mae: 48.079874, mean_q: 98.268818, mean_eps: 0.599298\n",
      "  8973/20000: episode: 261, duration: 0.190s, episode steps:  43, steps per second: 226, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 15.228926, mae: 47.926096, mean_q: 98.427841, mean_eps: 0.597205\n",
      "  8991/20000: episode: 262, duration: 0.074s, episode steps:  18, steps per second: 244, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 27.763785, mae: 48.571755, mean_q: 99.340607, mean_eps: 0.595832\n",
      "  9057/20000: episode: 263, duration: 0.263s, episode steps:  66, steps per second: 251, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 12.298348, mae: 48.644675, mean_q: 99.967862, mean_eps: 0.593943\n",
      "  9138/20000: episode: 264, duration: 0.345s, episode steps:  81, steps per second: 235, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 17.294073, mae: 48.739825, mean_q: 100.086582, mean_eps: 0.590635\n",
      "  9155/20000: episode: 265, duration: 0.069s, episode steps:  17, steps per second: 245, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 11.172499, mae: 48.853994, mean_q: 100.849972, mean_eps: 0.588430\n",
      "  9251/20000: episode: 266, duration: 0.412s, episode steps:  96, steps per second: 233, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.474638, mae: 49.609429, mean_q: 101.369140, mean_eps: 0.585888\n",
      "  9283/20000: episode: 267, duration: 0.128s, episode steps:  32, steps per second: 250, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 12.183132, mae: 50.373345, mean_q: 102.953788, mean_eps: 0.583007\n",
      "  9298/20000: episode: 268, duration: 0.068s, episode steps:  15, steps per second: 222, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 13.659115, mae: 50.124123, mean_q: 103.408353, mean_eps: 0.581950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9369/20000: episode: 269, duration: 0.337s, episode steps:  71, steps per second: 211, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 13.479184, mae: 50.397431, mean_q: 103.183453, mean_eps: 0.580015\n",
      "  9518/20000: episode: 270, duration: 0.588s, episode steps: 149, steps per second: 253, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 15.691347, mae: 50.748454, mean_q: 103.891487, mean_eps: 0.575065\n",
      "  9621/20000: episode: 271, duration: 0.406s, episode steps: 103, steps per second: 253, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 16.076225, mae: 51.538107, mean_q: 105.547430, mean_eps: 0.569395\n",
      "  9724/20000: episode: 272, duration: 0.404s, episode steps: 103, steps per second: 255, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 18.460935, mae: 52.119537, mean_q: 106.618945, mean_eps: 0.564760\n",
      "  9800/20000: episode: 273, duration: 0.299s, episode steps:  76, steps per second: 254, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.218164, mae: 53.183335, mean_q: 109.073165, mean_eps: 0.560732\n",
      "  9894/20000: episode: 274, duration: 0.410s, episode steps:  94, steps per second: 229, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 14.423724, mae: 53.319857, mean_q: 109.518105, mean_eps: 0.556907\n",
      " 10050/20000: episode: 275, duration: 0.614s, episode steps: 156, steps per second: 254, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 18.172516, mae: 53.967279, mean_q: 110.630175, mean_eps: 0.551283\n",
      " 10160/20000: episode: 276, duration: 0.448s, episode steps: 110, steps per second: 245, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 17.325994, mae: 54.306344, mean_q: 111.355909, mean_eps: 0.545297\n",
      " 10257/20000: episode: 277, duration: 0.389s, episode steps:  97, steps per second: 250, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 19.295605, mae: 54.891326, mean_q: 112.663703, mean_eps: 0.540640\n",
      " 10269/20000: episode: 278, duration: 0.053s, episode steps:  12, steps per second: 228, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 24.971901, mae: 56.244525, mean_q: 114.514050, mean_eps: 0.538187\n",
      " 10420/20000: episode: 279, duration: 0.651s, episode steps: 151, steps per second: 232, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 16.083656, mae: 55.454431, mean_q: 113.978003, mean_eps: 0.534520\n",
      " 10620/20000: episode: 280, duration: 0.820s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 17.530462, mae: 56.466140, mean_q: 115.758104, mean_eps: 0.526622\n",
      " 10680/20000: episode: 281, duration: 0.243s, episode steps:  60, steps per second: 247, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 19.039992, mae: 57.253250, mean_q: 117.394860, mean_eps: 0.520773\n",
      " 10715/20000: episode: 282, duration: 0.155s, episode steps:  35, steps per second: 225, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 19.980178, mae: 56.787273, mean_q: 116.848000, mean_eps: 0.518635\n",
      " 10813/20000: episode: 283, duration: 0.406s, episode steps:  98, steps per second: 241, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.884050, mae: 57.791343, mean_q: 118.350195, mean_eps: 0.515643\n",
      " 10938/20000: episode: 284, duration: 0.538s, episode steps: 125, steps per second: 232, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 16.174876, mae: 57.641794, mean_q: 118.277393, mean_eps: 0.510625\n",
      " 11138/20000: episode: 285, duration: 0.816s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 18.195787, mae: 58.745919, mean_q: 120.311189, mean_eps: 0.503312\n",
      " 11176/20000: episode: 286, duration: 0.157s, episode steps:  38, steps per second: 242, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 16.375666, mae: 58.852914, mean_q: 121.289433, mean_eps: 0.497958\n",
      " 11319/20000: episode: 287, duration: 0.579s, episode steps: 143, steps per second: 247, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 23.076702, mae: 60.176199, mean_q: 123.348122, mean_eps: 0.493885\n",
      " 11401/20000: episode: 288, duration: 0.325s, episode steps:  82, steps per second: 252, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 18.774879, mae: 60.737256, mean_q: 124.028774, mean_eps: 0.488822\n",
      " 11524/20000: episode: 289, duration: 0.515s, episode steps: 123, steps per second: 239, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 21.036086, mae: 61.096242, mean_q: 124.841641, mean_eps: 0.484210\n",
      " 11699/20000: episode: 290, duration: 0.702s, episode steps: 175, steps per second: 249, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 25.138674, mae: 61.698134, mean_q: 126.021197, mean_eps: 0.477505\n",
      " 11899/20000: episode: 291, duration: 0.815s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 19.497354, mae: 62.013469, mean_q: 126.595432, mean_eps: 0.469068\n",
      " 11988/20000: episode: 292, duration: 0.360s, episode steps:  89, steps per second: 247, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 18.461343, mae: 63.155838, mean_q: 129.258865, mean_eps: 0.462565\n",
      " 12058/20000: episode: 293, duration: 0.331s, episode steps:  70, steps per second: 212, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 32.559615, mae: 62.413287, mean_q: 127.595266, mean_eps: 0.458988\n",
      " 12240/20000: episode: 294, duration: 0.753s, episode steps: 182, steps per second: 242, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 17.435255, mae: 63.271555, mean_q: 129.598511, mean_eps: 0.453317\n",
      " 12354/20000: episode: 295, duration: 0.470s, episode steps: 114, steps per second: 242, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.483947, mae: 63.150815, mean_q: 129.154030, mean_eps: 0.446657\n",
      " 12496/20000: episode: 296, duration: 0.582s, episode steps: 142, steps per second: 244, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 25.229791, mae: 62.689583, mean_q: 128.418123, mean_eps: 0.440897\n",
      " 12664/20000: episode: 297, duration: 0.708s, episode steps: 168, steps per second: 237, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 26.267818, mae: 62.825016, mean_q: 128.307903, mean_eps: 0.433922\n",
      " 12803/20000: episode: 298, duration: 0.584s, episode steps: 139, steps per second: 238, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 24.390723, mae: 63.247161, mean_q: 129.107177, mean_eps: 0.427015\n",
      " 12950/20000: episode: 299, duration: 0.601s, episode steps: 147, steps per second: 244, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 21.194308, mae: 63.932238, mean_q: 130.814932, mean_eps: 0.420580\n",
      " 13125/20000: episode: 300, duration: 0.859s, episode steps: 175, steps per second: 204, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 19.458178, mae: 64.913897, mean_q: 132.863614, mean_eps: 0.413335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13236/20000: episode: 301, duration: 0.533s, episode steps: 111, steps per second: 208, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 17.618343, mae: 65.023665, mean_q: 132.928411, mean_eps: 0.406900\n",
      " 13410/20000: episode: 302, duration: 0.840s, episode steps: 174, steps per second: 207, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 24.311685, mae: 65.816336, mean_q: 134.483446, mean_eps: 0.400487\n",
      " 13562/20000: episode: 303, duration: 0.931s, episode steps: 152, steps per second: 163, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 23.398254, mae: 65.993668, mean_q: 134.398715, mean_eps: 0.393152\n",
      " 13755/20000: episode: 304, duration: 1.071s, episode steps: 193, steps per second: 180, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 20.965515, mae: 65.659036, mean_q: 134.054316, mean_eps: 0.385390\n",
      " 13886/20000: episode: 305, duration: 0.675s, episode steps: 131, steps per second: 194, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 14.723095, mae: 66.051369, mean_q: 134.965130, mean_eps: 0.378100\n",
      " 14022/20000: episode: 306, duration: 0.584s, episode steps: 136, steps per second: 233, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 22.929792, mae: 66.348088, mean_q: 135.413935, mean_eps: 0.372092\n",
      " 14222/20000: episode: 307, duration: 0.834s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 31.551162, mae: 66.817292, mean_q: 136.265642, mean_eps: 0.364532\n",
      " 14422/20000: episode: 308, duration: 0.799s, episode steps: 200, steps per second: 250, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 20.316593, mae: 66.990195, mean_q: 137.124709, mean_eps: 0.355532\n",
      " 14622/20000: episode: 309, duration: 0.831s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 25.139385, mae: 67.616688, mean_q: 138.054193, mean_eps: 0.346533\n",
      " 14816/20000: episode: 310, duration: 0.813s, episode steps: 194, steps per second: 239, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 29.090825, mae: 67.395519, mean_q: 137.450092, mean_eps: 0.337667\n",
      " 14903/20000: episode: 311, duration: 0.360s, episode steps:  87, steps per second: 242, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 21.858287, mae: 68.590012, mean_q: 140.070870, mean_eps: 0.331345\n",
      " 15047/20000: episode: 312, duration: 0.602s, episode steps: 144, steps per second: 239, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 30.626734, mae: 68.382183, mean_q: 138.991848, mean_eps: 0.326147\n",
      " 15198/20000: episode: 313, duration: 0.598s, episode steps: 151, steps per second: 253, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 25.434813, mae: 69.176573, mean_q: 141.222080, mean_eps: 0.319510\n",
      " 15294/20000: episode: 314, duration: 0.387s, episode steps:  96, steps per second: 248, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 32.092852, mae: 70.106159, mean_q: 142.921905, mean_eps: 0.313952\n",
      " 15485/20000: episode: 315, duration: 0.754s, episode steps: 191, steps per second: 253, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 23.157289, mae: 69.614894, mean_q: 141.715235, mean_eps: 0.307495\n",
      " 15672/20000: episode: 316, duration: 0.778s, episode steps: 187, steps per second: 240, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 34.560843, mae: 68.473112, mean_q: 139.536496, mean_eps: 0.298990\n",
      " 15848/20000: episode: 317, duration: 0.711s, episode steps: 176, steps per second: 247, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 27.812992, mae: 67.490530, mean_q: 137.866473, mean_eps: 0.290822\n",
      " 16011/20000: episode: 318, duration: 0.679s, episode steps: 163, steps per second: 240, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 18.760973, mae: 68.175473, mean_q: 138.926382, mean_eps: 0.283195\n",
      " 16164/20000: episode: 319, duration: 0.660s, episode steps: 153, steps per second: 232, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 22.383045, mae: 68.210743, mean_q: 138.966876, mean_eps: 0.276085\n",
      " 16315/20000: episode: 320, duration: 0.611s, episode steps: 151, steps per second: 247, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 21.405027, mae: 68.289589, mean_q: 139.074378, mean_eps: 0.269245\n",
      " 16515/20000: episode: 321, duration: 0.793s, episode steps: 200, steps per second: 252, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 19.880124, mae: 68.116972, mean_q: 138.677495, mean_eps: 0.261347\n",
      " 16715/20000: episode: 322, duration: 0.834s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.797645, mae: 67.902536, mean_q: 138.123470, mean_eps: 0.252347\n",
      " 16913/20000: episode: 323, duration: 0.786s, episode steps: 198, steps per second: 252, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 16.588670, mae: 67.925768, mean_q: 138.339553, mean_eps: 0.243392\n",
      " 17080/20000: episode: 324, duration: 0.659s, episode steps: 167, steps per second: 253, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 16.711997, mae: 68.440209, mean_q: 139.109534, mean_eps: 0.235180\n",
      " 17280/20000: episode: 325, duration: 0.855s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 27.050749, mae: 67.876458, mean_q: 138.003661, mean_eps: 0.226922\n",
      " 17458/20000: episode: 326, duration: 0.739s, episode steps: 178, steps per second: 241, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 17.627450, mae: 68.239654, mean_q: 139.042081, mean_eps: 0.218417\n",
      " 17633/20000: episode: 327, duration: 0.697s, episode steps: 175, steps per second: 251, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 25.347988, mae: 69.022113, mean_q: 140.536430, mean_eps: 0.210475\n",
      " 17750/20000: episode: 328, duration: 0.508s, episode steps: 117, steps per second: 230, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 13.050307, mae: 68.295543, mean_q: 138.942900, mean_eps: 0.203905\n",
      " 17916/20000: episode: 329, duration: 0.657s, episode steps: 166, steps per second: 253, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 25.521745, mae: 68.177451, mean_q: 138.456601, mean_eps: 0.197537\n",
      " 18100/20000: episode: 330, duration: 0.721s, episode steps: 184, steps per second: 255, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 26.917796, mae: 67.548662, mean_q: 137.256008, mean_eps: 0.189662\n",
      " 18300/20000: episode: 331, duration: 0.803s, episode steps: 200, steps per second: 249, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 19.439892, mae: 67.557076, mean_q: 137.525292, mean_eps: 0.181022\n",
      " 18500/20000: episode: 332, duration: 0.850s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 13.563594, mae: 67.664920, mean_q: 137.766772, mean_eps: 0.172022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18692/20000: episode: 333, duration: 0.765s, episode steps: 192, steps per second: 251, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 25.096328, mae: 67.915393, mean_q: 138.002484, mean_eps: 0.163202\n",
      " 18890/20000: episode: 334, duration: 0.811s, episode steps: 198, steps per second: 244, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 24.817604, mae: 66.984147, mean_q: 136.211657, mean_eps: 0.154427\n",
      " 19090/20000: episode: 335, duration: 0.789s, episode steps: 200, steps per second: 253, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 23.955387, mae: 66.850521, mean_q: 135.666890, mean_eps: 0.145472\n",
      " 19290/20000: episode: 336, duration: 0.785s, episode steps: 200, steps per second: 255, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 14.778563, mae: 66.719074, mean_q: 135.573861, mean_eps: 0.136472\n",
      " 19490/20000: episode: 337, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 19.423384, mae: 67.308890, mean_q: 136.467244, mean_eps: 0.127472\n",
      " 19690/20000: episode: 338, duration: 0.812s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 38.458367, mae: 66.202464, mean_q: 134.169182, mean_eps: 0.118472\n",
      " 19890/20000: episode: 339, duration: 0.810s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 25.455234, mae: 66.939349, mean_q: 135.643393, mean_eps: 0.109472\n",
      "done, took 86.279 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2add60a6088>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20_000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UDkEMp6wcpPY"
   },
   "outputs": [],
   "source": [
    "dqn.save_weights(\"my_weights_cartpole.h5f\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbElTOT7e4rt"
   },
   "outputs": [],
   "source": [
    "# If you want to load a trained model\n",
    "# dqn.load_weights(\"\"my_weights_cartpole.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0BdI9E8he-RN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
