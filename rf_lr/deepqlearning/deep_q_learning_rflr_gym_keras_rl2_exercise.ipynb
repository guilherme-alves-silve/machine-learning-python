{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advisory-score",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'>http://www.pieriandata.com</a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-agency",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement your first keras-rl agent based on the **Acrobot** environment (https://gym.openai.com/envs/Acrobot-v1/) <br />\n",
    "The goal of this environment is to maneuver the robot arm upwards above the line with as little steps as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-conducting",
   "metadata": {},
   "source": [
    "**TASK: Import necessary libraries** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import logger as gymlogger\n",
    "from IPython.display import clear_output\n",
    "\n",
    "gymlogger.set_level(40) # error only\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d7f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46926d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "**TASK: Create the environment** <br />\n",
    "The name is: *Acrobot-v1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4aa584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_environment(array):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(environment)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf70c4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for step in range(50):\n",
    "  environment = env.render(mode=\"rgb_array\")\n",
    "  show_environment(environment)\n",
    "  action = env.action_space.sample()\n",
    "  env.step(action)\n",
    "  clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: (6,)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == (6,) , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent** <br />\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want! <br />\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + num_observations))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mixture",
   "metadata": {},
   "source": [
    "**TASK: Initialize the circular buffer**<br />\n",
    "Make sure you set the limit appropriately (50000 works well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "\n",
    "memory = SequentialMemory(limit=80_000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-grain",
   "metadata": {},
   "source": [
    "**TASK: Use the epsilon greedy action selection strategy with *decaying* epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr=\"eps\",\n",
    "                              value_min=0.1,\n",
    "                              value_max=1.0,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-straight",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=1000, *target_model_update*=1000, *batch_size*=32 and *gamma*=0.99 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
    "               memory=memory, nb_steps_warmup=1000,\n",
    "               target_model_update=1000, batch_size=64,\n",
    "               gamma=0.99, policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-shooting",
   "metadata": {},
   "source": [
    "**TASK: Compile the model** <br />\n",
    "Feel free to explore the effects of different optimizers and learning rates.\n",
    "You can try Adam with a learning rate of 1e-3 as a first guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "**TASK: Fit the model** <br />\n",
    "150,000 steps should be a very good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adverse-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "   500/100000: episode: 1, duration: 0.362s, episode steps: 500, steps per second: 1382, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1000/100000: episode: 2, duration: 0.304s, episode steps: 500, steps per second: 1644, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1500/100000: episode: 3, duration: 4.740s, episode steps: 500, steps per second: 105, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.009523, mae: 0.549917, mean_q: -0.757981, mean_eps: 0.988750\n",
      "  2000/100000: episode: 4, duration: 4.186s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.902 [0.000, 2.000],  loss: 0.000338, mae: 0.545162, mean_q: -0.787797, mean_eps: 0.984255\n",
      "  2500/100000: episode: 5, duration: 4.187s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.008003, mae: 1.324911, mean_q: -1.936364, mean_eps: 0.979754\n",
      "  3000/100000: episode: 6, duration: 4.154s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.000961, mae: 1.320556, mean_q: -1.952065, mean_eps: 0.975254\n",
      "  3500/100000: episode: 7, duration: 4.342s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.010981, mae: 2.110278, mean_q: -3.109533, mean_eps: 0.970754\n",
      "  4000/100000: episode: 8, duration: 4.143s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.003716, mae: 2.106095, mean_q: -3.120371, mean_eps: 0.966255\n",
      "  4500/100000: episode: 9, duration: 4.338s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.018518, mae: 2.889328, mean_q: -4.259528, mean_eps: 0.961754\n",
      "  5000/100000: episode: 10, duration: 4.318s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.007756, mae: 2.884751, mean_q: -4.273976, mean_eps: 0.957255\n",
      "  5500/100000: episode: 11, duration: 4.277s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.023239, mae: 3.686865, mean_q: -5.450763, mean_eps: 0.952755\n",
      "  6000/100000: episode: 12, duration: 4.318s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.016132, mae: 3.675491, mean_q: -5.444244, mean_eps: 0.948255\n",
      "  6500/100000: episode: 13, duration: 4.135s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.033439, mae: 4.460968, mean_q: -6.591907, mean_eps: 0.943755\n",
      "  7000/100000: episode: 14, duration: 4.177s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.029056, mae: 4.454481, mean_q: -6.588168, mean_eps: 0.939254\n",
      "  7500/100000: episode: 15, duration: 4.235s, episode steps: 500, steps per second: 118, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.043457, mae: 5.211096, mean_q: -7.696481, mean_eps: 0.934755\n",
      "  8000/100000: episode: 16, duration: 4.203s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.037834, mae: 5.207587, mean_q: -7.708776, mean_eps: 0.930254\n",
      "  8500/100000: episode: 17, duration: 4.138s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.051358, mae: 5.840144, mean_q: -8.636847, mean_eps: 0.925755\n",
      "  9000/100000: episode: 18, duration: 4.157s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.054497, mae: 5.835104, mean_q: -8.630728, mean_eps: 0.921254\n",
      "  9500/100000: episode: 19, duration: 4.136s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.079965, mae: 6.506963, mean_q: -9.607645, mean_eps: 0.916755\n",
      " 10000/100000: episode: 20, duration: 4.150s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.063920, mae: 6.492836, mean_q: -9.601504, mean_eps: 0.912254\n",
      " 10500/100000: episode: 21, duration: 4.195s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.087244, mae: 7.271823, mean_q: -10.747432, mean_eps: 0.907755\n",
      " 11000/100000: episode: 22, duration: 4.138s, episode steps: 500, steps per second: 121, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.076539, mae: 7.265448, mean_q: -10.753441, mean_eps: 0.903254\n",
      " 11500/100000: episode: 23, duration: 4.227s, episode steps: 500, steps per second: 118, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.111260, mae: 7.872615, mean_q: -11.638726, mean_eps: 0.898755\n",
      " 12000/100000: episode: 24, duration: 4.298s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.103674, mae: 7.868589, mean_q: -11.642281, mean_eps: 0.894254\n",
      " 12500/100000: episode: 25, duration: 4.364s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.125572, mae: 8.494974, mean_q: -12.561417, mean_eps: 0.889755\n",
      " 13000/100000: episode: 26, duration: 4.218s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000],  loss: 0.115756, mae: 8.494642, mean_q: -12.571633, mean_eps: 0.885255\n",
      " 13500/100000: episode: 27, duration: 4.238s, episode steps: 500, steps per second: 118, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.135554, mae: 9.255745, mean_q: -13.690152, mean_eps: 0.880754\n",
      " 14000/100000: episode: 28, duration: 4.292s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.139945, mae: 9.250224, mean_q: -13.678951, mean_eps: 0.876255\n",
      " 14500/100000: episode: 29, duration: 4.284s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.188266, mae: 9.934563, mean_q: -14.679409, mean_eps: 0.871754\n",
      " 14978/100000: episode: 30, duration: 4.311s, episode steps: 478, steps per second: 111, episode reward: -477.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.173899, mae: 9.933163, mean_q: -14.683471, mean_eps: 0.867354\n",
      " 15478/100000: episode: 31, duration: 4.607s, episode steps: 500, steps per second: 109, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.215351, mae: 10.413657, mean_q: -15.363809, mean_eps: 0.862953\n",
      " 15978/100000: episode: 32, duration: 4.428s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.189578, mae: 10.436388, mean_q: -15.419327, mean_eps: 0.858453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16478/100000: episode: 33, duration: 4.331s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.222169, mae: 10.899908, mean_q: -16.101049, mean_eps: 0.853953\n",
      " 16978/100000: episode: 34, duration: 4.268s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.194304, mae: 10.916436, mean_q: -16.152262, mean_eps: 0.849452\n",
      " 17478/100000: episode: 35, duration: 4.282s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.222930, mae: 11.346473, mean_q: -16.759635, mean_eps: 0.844952\n",
      " 17978/100000: episode: 36, duration: 4.261s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.227171, mae: 11.354349, mean_q: -16.790988, mean_eps: 0.840452\n",
      " 18478/100000: episode: 37, duration: 4.356s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.196583, mae: 11.673617, mean_q: -17.264213, mean_eps: 0.835952\n",
      " 18978/100000: episode: 38, duration: 4.216s, episode steps: 500, steps per second: 119, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.210169, mae: 11.673999, mean_q: -17.272203, mean_eps: 0.831453\n",
      " 19478/100000: episode: 39, duration: 4.420s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.272949, mae: 12.177377, mean_q: -17.992887, mean_eps: 0.826952\n",
      " 19881/100000: episode: 40, duration: 3.476s, episode steps: 403, steps per second: 116, episode reward: -402.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.295714, mae: 12.184548, mean_q: -17.994453, mean_eps: 0.822889\n",
      " 20381/100000: episode: 41, duration: 4.308s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.337470, mae: 12.584722, mean_q: -18.575114, mean_eps: 0.818825\n",
      " 20877/100000: episode: 42, duration: 4.126s, episode steps: 496, steps per second: 120, episode reward: -495.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.223815, mae: 12.691183, mean_q: -18.771396, mean_eps: 0.814343\n",
      " 21320/100000: episode: 43, duration: 3.783s, episode steps: 443, steps per second: 117, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.299643, mae: 13.005148, mean_q: -19.203862, mean_eps: 0.810118\n",
      " 21735/100000: episode: 44, duration: 3.520s, episode steps: 415, steps per second: 118, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.273488, mae: 13.089239, mean_q: -19.329565, mean_eps: 0.806257\n",
      " 22235/100000: episode: 45, duration: 4.670s, episode steps: 500, steps per second: 107, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.862 [0.000, 2.000],  loss: 0.413111, mae: 13.244126, mean_q: -19.520247, mean_eps: 0.802140\n",
      " 22521/100000: episode: 46, duration: 2.607s, episode steps: 286, steps per second: 110, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.389932, mae: 13.394826, mean_q: -19.710130, mean_eps: 0.798602\n",
      " 22878/100000: episode: 47, duration: 3.237s, episode steps: 357, steps per second: 110, episode reward: -356.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.337387, mae: 13.392745, mean_q: -19.721061, mean_eps: 0.795709\n",
      " 23378/100000: episode: 48, duration: 4.512s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.387104, mae: 13.719732, mean_q: -20.209154, mean_eps: 0.791852\n",
      " 23878/100000: episode: 49, duration: 4.292s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.361280, mae: 13.819113, mean_q: -20.357474, mean_eps: 0.787353\n",
      " 24378/100000: episode: 50, duration: 4.491s, episode steps: 500, steps per second: 111, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.383559, mae: 14.058989, mean_q: -20.690340, mean_eps: 0.782852\n",
      " 24699/100000: episode: 51, duration: 2.867s, episode steps: 321, steps per second: 112, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.395308, mae: 14.136571, mean_q: -20.824954, mean_eps: 0.779158\n",
      " 25128/100000: episode: 52, duration: 3.808s, episode steps: 429, steps per second: 113, episode reward: -428.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.544087, mae: 14.225047, mean_q: -20.904366, mean_eps: 0.775783\n",
      " 25434/100000: episode: 53, duration: 2.686s, episode steps: 306, steps per second: 114, episode reward: -305.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.438373, mae: 14.532861, mean_q: -21.375071, mean_eps: 0.772475\n",
      " 25934/100000: episode: 54, duration: 4.436s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.400175, mae: 14.537324, mean_q: -21.410306, mean_eps: 0.768849\n",
      " 26291/100000: episode: 55, duration: 3.188s, episode steps: 357, steps per second: 112, episode reward: -356.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.511455, mae: 14.852131, mean_q: -21.831852, mean_eps: 0.764992\n",
      " 26706/100000: episode: 56, duration: 3.553s, episode steps: 415, steps per second: 117, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.449465, mae: 14.881807, mean_q: -21.897299, mean_eps: 0.761518\n",
      " 27070/100000: episode: 57, duration: 3.184s, episode steps: 364, steps per second: 114, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.588983, mae: 14.939221, mean_q: -21.944287, mean_eps: 0.758013\n",
      " 27570/100000: episode: 58, duration: 4.354s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.518537, mae: 15.171898, mean_q: -22.324517, mean_eps: 0.754124\n",
      " 27995/100000: episode: 59, duration: 3.763s, episode steps: 425, steps per second: 113, episode reward: -424.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.416686, mae: 15.138135, mean_q: -22.294677, mean_eps: 0.749962\n",
      " 28365/100000: episode: 60, duration: 3.278s, episode steps: 370, steps per second: 113, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.561862, mae: 15.549572, mean_q: -22.853366, mean_eps: 0.746385\n",
      " 28723/100000: episode: 61, duration: 3.052s, episode steps: 358, steps per second: 117, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.516459, mae: 15.508521, mean_q: -22.818909, mean_eps: 0.743109\n",
      " 29223/100000: episode: 62, duration: 4.402s, episode steps: 500, steps per second: 114, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.447078, mae: 15.667527, mean_q: -23.057441, mean_eps: 0.739247\n",
      " 29635/100000: episode: 63, duration: 3.609s, episode steps: 412, steps per second: 114, episode reward: -411.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.413620, mae: 15.832816, mean_q: -23.314480, mean_eps: 0.735144\n",
      " 29943/100000: episode: 64, duration: 2.723s, episode steps: 308, steps per second: 113, episode reward: -307.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.428990, mae: 15.809240, mean_q: -23.276361, mean_eps: 0.731904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30363/100000: episode: 65, duration: 3.729s, episode steps: 420, steps per second: 113, episode reward: -419.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 0.410889, mae: 16.100549, mean_q: -23.672829, mean_eps: 0.728627\n",
      " 30672/100000: episode: 66, duration: 2.715s, episode steps: 309, steps per second: 114, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.507344, mae: 16.078592, mean_q: -23.604595, mean_eps: 0.725347\n",
      " 31172/100000: episode: 67, duration: 4.306s, episode steps: 500, steps per second: 116, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.518307, mae: 16.184088, mean_q: -23.767414, mean_eps: 0.721707\n",
      " 31405/100000: episode: 68, duration: 2.005s, episode steps: 233, steps per second: 116, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.431935, mae: 16.318621, mean_q: -23.981818, mean_eps: 0.718408\n",
      " 31724/100000: episode: 69, duration: 2.693s, episode steps: 319, steps per second: 118, episode reward: -318.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.449750, mae: 16.292523, mean_q: -23.940910, mean_eps: 0.715924\n",
      " 31970/100000: episode: 70, duration: 2.103s, episode steps: 246, steps per second: 117, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.400118, mae: 16.323370, mean_q: -23.981194, mean_eps: 0.713382\n",
      " 32326/100000: episode: 71, duration: 3.097s, episode steps: 356, steps per second: 115, episode reward: -355.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.641494, mae: 16.650284, mean_q: -24.449455, mean_eps: 0.710673\n",
      " 32769/100000: episode: 72, duration: 3.882s, episode steps: 443, steps per second: 114, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.403439, mae: 16.661952, mean_q: -24.497204, mean_eps: 0.707077\n",
      " 33062/100000: episode: 73, duration: 2.515s, episode steps: 293, steps per second: 116, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.799 [0.000, 2.000],  loss: 0.558038, mae: 16.715771, mean_q: -24.549031, mean_eps: 0.703765\n",
      " 33254/100000: episode: 74, duration: 1.785s, episode steps: 192, steps per second: 108, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.496619, mae: 16.819512, mean_q: -24.714872, mean_eps: 0.701582\n",
      " 33577/100000: episode: 75, duration: 3.132s, episode steps: 323, steps per second: 103, episode reward: -322.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.507323, mae: 16.840486, mean_q: -24.786516, mean_eps: 0.699265\n",
      " 33932/100000: episode: 76, duration: 3.366s, episode steps: 355, steps per second: 105, episode reward: -354.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.525526, mae: 16.828515, mean_q: -24.749698, mean_eps: 0.696214\n",
      " 34281/100000: episode: 77, duration: 3.324s, episode steps: 349, steps per second: 105, episode reward: -348.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.406766, mae: 17.039608, mean_q: -25.082171, mean_eps: 0.693046\n",
      " 34781/100000: episode: 78, duration: 4.414s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.554765, mae: 17.096002, mean_q: -25.150898, mean_eps: 0.689226\n",
      " 35186/100000: episode: 79, duration: 3.473s, episode steps: 405, steps per second: 117, episode reward: -404.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.487023, mae: 17.214130, mean_q: -25.344433, mean_eps: 0.685153\n",
      " 35377/100000: episode: 80, duration: 1.650s, episode steps: 191, steps per second: 116, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 0.544900, mae: 17.307488, mean_q: -25.466435, mean_eps: 0.682471\n",
      " 35748/100000: episode: 81, duration: 3.224s, episode steps: 371, steps per second: 115, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.578342, mae: 17.307311, mean_q: -25.457531, mean_eps: 0.679942\n",
      " 36060/100000: episode: 82, duration: 2.702s, episode steps: 312, steps per second: 115, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.593229, mae: 17.389037, mean_q: -25.578727, mean_eps: 0.676868\n",
      " 36333/100000: episode: 83, duration: 2.402s, episode steps: 273, steps per second: 114, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.545815, mae: 17.751076, mean_q: -26.111721, mean_eps: 0.674236\n",
      " 36562/100000: episode: 84, duration: 2.057s, episode steps: 229, steps per second: 111, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.514226, mae: 17.710825, mean_q: -26.083068, mean_eps: 0.671977\n",
      " 36829/100000: episode: 85, duration: 2.295s, episode steps: 267, steps per second: 116, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 0.480529, mae: 17.772660, mean_q: -26.167643, mean_eps: 0.669745\n",
      " 37216/100000: episode: 86, duration: 3.577s, episode steps: 387, steps per second: 108, episode reward: -386.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.497212, mae: 17.883309, mean_q: -26.316787, mean_eps: 0.666802\n",
      " 37503/100000: episode: 87, duration: 2.649s, episode steps: 287, steps per second: 108, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.559110, mae: 18.034150, mean_q: -26.529514, mean_eps: 0.663769\n",
      " 37901/100000: episode: 88, duration: 3.683s, episode steps: 398, steps per second: 108, episode reward: -397.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.506009, mae: 17.929921, mean_q: -26.390072, mean_eps: 0.660686\n",
      " 38171/100000: episode: 89, duration: 2.647s, episode steps: 270, steps per second: 102, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.567277, mae: 17.967547, mean_q: -26.436389, mean_eps: 0.657681\n",
      " 38441/100000: episode: 90, duration: 2.411s, episode steps: 270, steps per second: 112, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.661706, mae: 17.990705, mean_q: -26.479043, mean_eps: 0.655251\n",
      " 38623/100000: episode: 91, duration: 1.563s, episode steps: 182, steps per second: 116, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.425064, mae: 18.000798, mean_q: -26.509711, mean_eps: 0.653216\n",
      " 38927/100000: episode: 92, duration: 2.745s, episode steps: 304, steps per second: 111, episode reward: -303.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.499888, mae: 17.964263, mean_q: -26.447187, mean_eps: 0.651030\n",
      " 39193/100000: episode: 93, duration: 2.371s, episode steps: 266, steps per second: 112, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.537656, mae: 18.357325, mean_q: -27.012405, mean_eps: 0.648464\n",
      " 39469/100000: episode: 94, duration: 2.319s, episode steps: 276, steps per second: 119, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.618218, mae: 18.428068, mean_q: -27.093036, mean_eps: 0.646026\n",
      " 39967/100000: episode: 95, duration: 4.188s, episode steps: 498, steps per second: 119, episode reward: -497.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.526218, mae: 18.417168, mean_q: -27.100323, mean_eps: 0.642543\n",
      " 40194/100000: episode: 96, duration: 2.066s, episode steps: 227, steps per second: 110, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.463089, mae: 18.709170, mean_q: -27.531029, mean_eps: 0.639280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40454/100000: episode: 97, duration: 2.266s, episode steps: 260, steps per second: 115, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.701624, mae: 18.724489, mean_q: -27.539473, mean_eps: 0.637089\n",
      " 40800/100000: episode: 98, duration: 3.027s, episode steps: 346, steps per second: 114, episode reward: -345.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.659980, mae: 18.734180, mean_q: -27.556273, mean_eps: 0.634362\n",
      " 41035/100000: episode: 99, duration: 2.053s, episode steps: 235, steps per second: 114, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.567398, mae: 18.789536, mean_q: -27.627231, mean_eps: 0.631747\n",
      " 41535/100000: episode: 100, duration: 4.334s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.470516, mae: 19.010123, mean_q: -27.988419, mean_eps: 0.628439\n",
      " 41807/100000: episode: 101, duration: 2.341s, episode steps: 272, steps per second: 116, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.757 [0.000, 2.000],  loss: 0.639165, mae: 18.978313, mean_q: -27.918532, mean_eps: 0.624965\n",
      " 42193/100000: episode: 102, duration: 3.314s, episode steps: 386, steps per second: 116, episode reward: -385.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.456732, mae: 19.172850, mean_q: -28.222688, mean_eps: 0.622004\n",
      " 42472/100000: episode: 103, duration: 2.531s, episode steps: 279, steps per second: 110, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.514725, mae: 19.319231, mean_q: -28.468056, mean_eps: 0.619012\n",
      " 42743/100000: episode: 104, duration: 2.373s, episode steps: 271, steps per second: 114, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.611759, mae: 19.309928, mean_q: -28.442394, mean_eps: 0.616537\n",
      " 43013/100000: episode: 105, duration: 2.309s, episode steps: 270, steps per second: 117, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.467506, mae: 19.219880, mean_q: -28.317261, mean_eps: 0.614103\n",
      " 43311/100000: episode: 106, duration: 2.559s, episode steps: 298, steps per second: 116, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.661757, mae: 19.465840, mean_q: -28.636994, mean_eps: 0.611546\n",
      " 43560/100000: episode: 107, duration: 2.159s, episode steps: 249, steps per second: 115, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.519099, mae: 19.475255, mean_q: -28.683661, mean_eps: 0.609085\n",
      " 43812/100000: episode: 108, duration: 2.258s, episode steps: 252, steps per second: 112, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.620492, mae: 19.437164, mean_q: -28.616442, mean_eps: 0.606831\n",
      " 44020/100000: episode: 109, duration: 1.899s, episode steps: 208, steps per second: 110, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 0.512302, mae: 19.404757, mean_q: -28.569625, mean_eps: 0.604761\n",
      " 44318/100000: episode: 110, duration: 2.697s, episode steps: 298, steps per second: 110, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.542646, mae: 19.840573, mean_q: -29.215115, mean_eps: 0.602483\n",
      " 44577/100000: episode: 111, duration: 2.231s, episode steps: 259, steps per second: 116, episode reward: -258.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.571523, mae: 19.799211, mean_q: -29.160126, mean_eps: 0.599977\n",
      " 44871/100000: episode: 112, duration: 2.482s, episode steps: 294, steps per second: 118, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.607577, mae: 19.808271, mean_q: -29.163602, mean_eps: 0.597488\n",
      " 45213/100000: episode: 113, duration: 2.928s, episode steps: 342, steps per second: 117, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.504692, mae: 19.889224, mean_q: -29.268618, mean_eps: 0.594626\n",
      " 45408/100000: episode: 114, duration: 1.755s, episode steps: 195, steps per second: 111, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.566515, mae: 19.916401, mean_q: -29.309335, mean_eps: 0.592210\n",
      " 45624/100000: episode: 115, duration: 1.922s, episode steps: 216, steps per second: 112, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.486244, mae: 19.938687, mean_q: -29.329709, mean_eps: 0.590360\n",
      " 45844/100000: episode: 116, duration: 2.030s, episode steps: 220, steps per second: 108, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.584040, mae: 20.020634, mean_q: -29.464564, mean_eps: 0.588399\n",
      " 46083/100000: episode: 117, duration: 2.235s, episode steps: 239, steps per second: 107, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.783161, mae: 19.976330, mean_q: -29.359015, mean_eps: 0.586333\n",
      " 46319/100000: episode: 118, duration: 2.246s, episode steps: 236, steps per second: 105, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.208 [0.000, 2.000],  loss: 0.593169, mae: 20.145436, mean_q: -29.622460, mean_eps: 0.584195\n",
      " 46490/100000: episode: 119, duration: 1.623s, episode steps: 171, steps per second: 105, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.480543, mae: 20.242792, mean_q: -29.793161, mean_eps: 0.582364\n",
      " 46670/100000: episode: 120, duration: 1.551s, episode steps: 180, steps per second: 116, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.728634, mae: 20.130046, mean_q: -29.578922, mean_eps: 0.580785\n",
      " 46890/100000: episode: 121, duration: 1.893s, episode steps: 220, steps per second: 116, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.435713, mae: 20.147715, mean_q: -29.657111, mean_eps: 0.578985\n",
      " 47133/100000: episode: 122, duration: 2.105s, episode steps: 243, steps per second: 115, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.550355, mae: 20.221334, mean_q: -29.732848, mean_eps: 0.576901\n",
      " 47393/100000: episode: 123, duration: 2.207s, episode steps: 260, steps per second: 118, episode reward: -259.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.407155, mae: 20.227662, mean_q: -29.726547, mean_eps: 0.574638\n",
      " 47808/100000: episode: 124, duration: 3.569s, episode steps: 415, steps per second: 116, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.609663, mae: 20.309364, mean_q: -29.855210, mean_eps: 0.571600\n",
      " 48041/100000: episode: 125, duration: 2.078s, episode steps: 233, steps per second: 112, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.581905, mae: 20.256584, mean_q: -29.767157, mean_eps: 0.568684\n",
      " 48295/100000: episode: 126, duration: 2.220s, episode steps: 254, steps per second: 114, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.388875, mae: 20.362927, mean_q: -29.925487, mean_eps: 0.566492\n",
      " 48615/100000: episode: 127, duration: 2.875s, episode steps: 320, steps per second: 111, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.642517, mae: 20.339480, mean_q: -29.868398, mean_eps: 0.563909\n",
      " 48795/100000: episode: 128, duration: 1.560s, episode steps: 180, steps per second: 115, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.419614, mae: 20.274384, mean_q: -29.800848, mean_eps: 0.561659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49065/100000: episode: 129, duration: 2.302s, episode steps: 270, steps per second: 117, episode reward: -269.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.666071, mae: 20.292625, mean_q: -29.791563, mean_eps: 0.559635\n",
      " 49263/100000: episode: 130, duration: 1.702s, episode steps: 198, steps per second: 116, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.479537, mae: 20.529947, mean_q: -30.165320, mean_eps: 0.557528\n",
      " 49461/100000: episode: 131, duration: 1.723s, episode steps: 198, steps per second: 115, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.746113, mae: 20.549974, mean_q: -30.169820, mean_eps: 0.555747\n",
      " 49710/100000: episode: 132, duration: 2.123s, episode steps: 249, steps per second: 117, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.636598, mae: 20.461660, mean_q: -30.023320, mean_eps: 0.553735\n",
      " 49929/100000: episode: 133, duration: 1.871s, episode steps: 219, steps per second: 117, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.725017, mae: 20.377574, mean_q: -29.910585, mean_eps: 0.551629\n",
      " 50198/100000: episode: 134, duration: 2.294s, episode steps: 269, steps per second: 117, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.803613, mae: 20.654184, mean_q: -30.318775, mean_eps: 0.549433\n",
      " 50394/100000: episode: 135, duration: 1.667s, episode steps: 196, steps per second: 118, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.696174, mae: 20.674372, mean_q: -30.366283, mean_eps: 0.547340\n",
      " 50601/100000: episode: 136, duration: 1.755s, episode steps: 207, steps per second: 118, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 0.588151, mae: 20.665722, mean_q: -30.355539, mean_eps: 0.545527\n",
      " 50760/100000: episode: 137, duration: 1.467s, episode steps: 159, steps per second: 108, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.519659, mae: 20.650892, mean_q: -30.359664, mean_eps: 0.543880\n",
      " 51010/100000: episode: 138, duration: 2.229s, episode steps: 250, steps per second: 112, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.678773, mae: 20.631816, mean_q: -30.310899, mean_eps: 0.542040\n",
      " 51202/100000: episode: 139, duration: 1.766s, episode steps: 192, steps per second: 109, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.624136, mae: 20.876405, mean_q: -30.653522, mean_eps: 0.540050\n",
      " 51380/100000: episode: 140, duration: 1.766s, episode steps: 178, steps per second: 101, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.596833, mae: 20.993522, mean_q: -30.855074, mean_eps: 0.538385\n",
      " 51673/100000: episode: 141, duration: 2.818s, episode steps: 293, steps per second: 104, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.500801, mae: 21.016250, mean_q: -30.886358, mean_eps: 0.536266\n",
      " 51856/100000: episode: 142, duration: 1.765s, episode steps: 183, steps per second: 104, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.636581, mae: 20.887276, mean_q: -30.678429, mean_eps: 0.534124\n",
      " 51996/100000: episode: 143, duration: 1.354s, episode steps: 140, steps per second: 103, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.662510, mae: 20.816475, mean_q: -30.577454, mean_eps: 0.532671\n",
      " 52212/100000: episode: 144, duration: 1.956s, episode steps: 216, steps per second: 110, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.491715, mae: 21.013700, mean_q: -30.869314, mean_eps: 0.531069\n",
      " 52386/100000: episode: 145, duration: 1.554s, episode steps: 174, steps per second: 112, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.544517, mae: 21.033578, mean_q: -30.916839, mean_eps: 0.529313\n",
      " 52620/100000: episode: 146, duration: 2.060s, episode steps: 234, steps per second: 114, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.554518, mae: 20.965911, mean_q: -30.821669, mean_eps: 0.527477\n",
      " 52904/100000: episode: 147, duration: 2.608s, episode steps: 284, steps per second: 109, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.536520, mae: 20.956527, mean_q: -30.796091, mean_eps: 0.525146\n",
      " 53040/100000: episode: 148, duration: 1.228s, episode steps: 136, steps per second: 111, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.510073, mae: 21.104800, mean_q: -31.009097, mean_eps: 0.523256\n",
      " 53243/100000: episode: 149, duration: 1.822s, episode steps: 203, steps per second: 111, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.426212, mae: 21.014369, mean_q: -30.900319, mean_eps: 0.521731\n",
      " 53448/100000: episode: 150, duration: 1.933s, episode steps: 205, steps per second: 106, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.578918, mae: 20.980210, mean_q: -30.836352, mean_eps: 0.519895\n",
      " 53634/100000: episode: 151, duration: 1.677s, episode steps: 186, steps per second: 111, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 0.711743, mae: 20.952510, mean_q: -30.795098, mean_eps: 0.518135\n",
      " 53801/100000: episode: 152, duration: 1.707s, episode steps: 167, steps per second:  98, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.494469, mae: 21.029054, mean_q: -30.935501, mean_eps: 0.516547\n",
      " 53965/100000: episode: 153, duration: 1.593s, episode steps: 164, steps per second: 103, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.476571, mae: 21.058837, mean_q: -30.954141, mean_eps: 0.515058\n",
      " 54150/100000: episode: 154, duration: 1.709s, episode steps: 185, steps per second: 108, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.421671, mae: 21.127102, mean_q: -31.048307, mean_eps: 0.513487\n",
      " 54326/100000: episode: 155, duration: 1.605s, episode steps: 176, steps per second: 110, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.670147, mae: 21.126463, mean_q: -31.025541, mean_eps: 0.511862\n",
      " 54518/100000: episode: 156, duration: 1.830s, episode steps: 192, steps per second: 105, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.798812, mae: 21.129306, mean_q: -31.031378, mean_eps: 0.510206\n",
      " 54737/100000: episode: 157, duration: 2.050s, episode steps: 219, steps per second: 107, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.635157, mae: 21.011337, mean_q: -30.855731, mean_eps: 0.508357\n",
      " 54911/100000: episode: 158, duration: 1.528s, episode steps: 174, steps per second: 114, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.489750, mae: 21.088288, mean_q: -31.009988, mean_eps: 0.506588\n",
      " 55116/100000: episode: 159, duration: 1.863s, episode steps: 205, steps per second: 110, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.526813, mae: 21.375209, mean_q: -31.416226, mean_eps: 0.504883\n",
      " 55356/100000: episode: 160, duration: 2.190s, episode steps: 240, steps per second: 110, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.559943, mae: 21.465163, mean_q: -31.541110, mean_eps: 0.502881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55475/100000: episode: 161, duration: 1.061s, episode steps: 119, steps per second: 112, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.701597, mae: 21.422847, mean_q: -31.476859, mean_eps: 0.501265\n",
      " 55719/100000: episode: 162, duration: 2.117s, episode steps: 244, steps per second: 115, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.801947, mae: 21.389848, mean_q: -31.413033, mean_eps: 0.499632\n",
      " 55893/100000: episode: 163, duration: 1.509s, episode steps: 174, steps per second: 115, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.428362, mae: 21.293401, mean_q: -31.311054, mean_eps: 0.497750\n",
      " 56066/100000: episode: 164, duration: 1.532s, episode steps: 173, steps per second: 113, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.521568, mae: 21.438347, mean_q: -31.504942, mean_eps: 0.496189\n",
      " 56317/100000: episode: 165, duration: 2.198s, episode steps: 251, steps per second: 114, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.703333, mae: 21.587018, mean_q: -31.710985, mean_eps: 0.494281\n",
      " 56529/100000: episode: 166, duration: 1.863s, episode steps: 212, steps per second: 114, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.499171, mae: 21.541336, mean_q: -31.676236, mean_eps: 0.492197\n",
      " 56660/100000: episode: 167, duration: 1.141s, episode steps: 131, steps per second: 115, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.719823, mae: 21.549913, mean_q: -31.623719, mean_eps: 0.490654\n",
      " 56942/100000: episode: 168, duration: 2.450s, episode steps: 282, steps per second: 115, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.643207, mae: 21.596137, mean_q: -31.738419, mean_eps: 0.488795\n",
      " 57129/100000: episode: 169, duration: 1.623s, episode steps: 187, steps per second: 115, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.593969, mae: 21.689435, mean_q: -31.873232, mean_eps: 0.486685\n",
      " 57314/100000: episode: 170, duration: 1.642s, episode steps: 185, steps per second: 113, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.487006, mae: 21.755747, mean_q: -31.949615, mean_eps: 0.485011\n",
      " 57460/100000: episode: 171, duration: 1.410s, episode steps: 146, steps per second: 104, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.626263, mae: 21.794908, mean_q: -32.015248, mean_eps: 0.483522\n",
      " 57636/100000: episode: 172, duration: 1.573s, episode steps: 176, steps per second: 112, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.689909, mae: 21.749531, mean_q: -31.937837, mean_eps: 0.482072\n",
      " 57804/100000: episode: 173, duration: 1.559s, episode steps: 168, steps per second: 108, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.491723, mae: 21.640235, mean_q: -31.794458, mean_eps: 0.480525\n",
      " 57945/100000: episode: 174, duration: 1.381s, episode steps: 141, steps per second: 102, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.443318, mae: 21.826951, mean_q: -32.067625, mean_eps: 0.479134\n",
      " 58126/100000: episode: 175, duration: 1.688s, episode steps: 181, steps per second: 107, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.645331, mae: 21.884551, mean_q: -32.149815, mean_eps: 0.477685\n",
      " 58241/100000: episode: 176, duration: 1.004s, episode steps: 115, steps per second: 115, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.577951, mae: 21.870396, mean_q: -32.122135, mean_eps: 0.476353\n",
      " 58430/100000: episode: 177, duration: 1.718s, episode steps: 189, steps per second: 110, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.469864, mae: 22.025584, mean_q: -32.367146, mean_eps: 0.474985\n",
      " 58594/100000: episode: 178, duration: 1.456s, episode steps: 164, steps per second: 113, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.557088, mae: 22.069753, mean_q: -32.436350, mean_eps: 0.473396\n",
      " 58738/100000: episode: 179, duration: 1.246s, episode steps: 144, steps per second: 116, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.604542, mae: 22.038912, mean_q: -32.369494, mean_eps: 0.472010\n",
      " 58911/100000: episode: 180, duration: 1.557s, episode steps: 173, steps per second: 111, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.479216, mae: 21.918017, mean_q: -32.172308, mean_eps: 0.470584\n",
      " 59111/100000: episode: 181, duration: 1.762s, episode steps: 200, steps per second: 114, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.456749, mae: 21.961098, mean_q: -32.244791, mean_eps: 0.468905\n",
      " 59290/100000: episode: 182, duration: 1.556s, episode steps: 179, steps per second: 115, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.455165, mae: 22.281106, mean_q: -32.742977, mean_eps: 0.467200\n",
      " 59403/100000: episode: 183, duration: 0.981s, episode steps: 113, steps per second: 115, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.425592, mae: 22.202871, mean_q: -32.640152, mean_eps: 0.465886\n",
      " 59510/100000: episode: 184, duration: 1.041s, episode steps: 107, steps per second: 103, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.497026, mae: 22.116995, mean_q: -32.514389, mean_eps: 0.464896\n",
      " 59649/100000: episode: 185, duration: 1.239s, episode steps: 139, steps per second: 112, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.656565, mae: 22.185909, mean_q: -32.579629, mean_eps: 0.463789\n",
      " 59789/100000: episode: 186, duration: 1.226s, episode steps: 140, steps per second: 114, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.574722, mae: 22.177976, mean_q: -32.576477, mean_eps: 0.462533\n",
      " 59948/100000: episode: 187, duration: 1.428s, episode steps: 159, steps per second: 111, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.661112, mae: 22.007846, mean_q: -32.325784, mean_eps: 0.461188\n",
      " 60079/100000: episode: 188, duration: 1.217s, episode steps: 131, steps per second: 108, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.518431, mae: 22.122659, mean_q: -32.466007, mean_eps: 0.459883\n",
      " 60279/100000: episode: 189, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.829151, mae: 22.300868, mean_q: -32.704298, mean_eps: 0.458393\n",
      " 60430/100000: episode: 190, duration: 1.442s, episode steps: 151, steps per second: 105, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.363465, mae: 22.331152, mean_q: -32.788466, mean_eps: 0.456814\n",
      " 60630/100000: episode: 191, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.759334, mae: 22.156743, mean_q: -32.492245, mean_eps: 0.455234\n",
      " 60799/100000: episode: 192, duration: 1.616s, episode steps: 169, steps per second: 105, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.842995, mae: 22.196138, mean_q: -32.548587, mean_eps: 0.453574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60993/100000: episode: 193, duration: 1.742s, episode steps: 194, steps per second: 111, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.539822, mae: 22.291902, mean_q: -32.727237, mean_eps: 0.451941\n",
      " 61156/100000: episode: 194, duration: 1.499s, episode steps: 163, steps per second: 109, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.560179, mae: 22.453977, mean_q: -32.916223, mean_eps: 0.450334\n",
      " 61322/100000: episode: 195, duration: 1.518s, episode steps: 166, steps per second: 109, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.365585, mae: 22.430430, mean_q: -32.948908, mean_eps: 0.448854\n",
      " 61446/100000: episode: 196, duration: 1.099s, episode steps: 124, steps per second: 113, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.485877, mae: 22.421030, mean_q: -32.927796, mean_eps: 0.447548\n",
      " 61599/100000: episode: 197, duration: 1.425s, episode steps: 153, steps per second: 107, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.528297, mae: 22.419293, mean_q: -32.939346, mean_eps: 0.446302\n",
      " 61743/100000: episode: 198, duration: 1.346s, episode steps: 144, steps per second: 107, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.471842, mae: 22.552532, mean_q: -33.113470, mean_eps: 0.444965\n",
      " 61878/100000: episode: 199, duration: 1.237s, episode steps: 135, steps per second: 109, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.531707, mae: 22.250067, mean_q: -32.656887, mean_eps: 0.443710\n",
      " 62007/100000: episode: 200, duration: 1.180s, episode steps: 129, steps per second: 109, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.733967, mae: 22.415580, mean_q: -32.890925, mean_eps: 0.442522\n",
      " 62200/100000: episode: 201, duration: 1.708s, episode steps: 193, steps per second: 113, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.797463, mae: 22.594917, mean_q: -33.114963, mean_eps: 0.441073\n",
      " 62373/100000: episode: 202, duration: 1.590s, episode steps: 173, steps per second: 109, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.535302, mae: 22.624713, mean_q: -33.202074, mean_eps: 0.439426\n",
      " 62503/100000: episode: 203, duration: 1.244s, episode steps: 130, steps per second: 104, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.768005, mae: 22.731862, mean_q: -33.366199, mean_eps: 0.438063\n",
      " 62694/100000: episode: 204, duration: 1.856s, episode steps: 191, steps per second: 103, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.537647, mae: 22.635767, mean_q: -33.204116, mean_eps: 0.436618\n",
      " 62849/100000: episode: 205, duration: 1.520s, episode steps: 155, steps per second: 102, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.417950, mae: 22.587186, mean_q: -33.168757, mean_eps: 0.435061\n",
      " 62981/100000: episode: 206, duration: 1.277s, episode steps: 132, steps per second: 103, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.515431, mae: 22.607551, mean_q: -33.168945, mean_eps: 0.433769\n",
      " 63096/100000: episode: 207, duration: 1.093s, episode steps: 115, steps per second: 105, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.617325, mae: 22.772471, mean_q: -33.400015, mean_eps: 0.432658\n",
      " 63248/100000: episode: 208, duration: 1.476s, episode steps: 152, steps per second: 103, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.591517, mae: 22.611320, mean_q: -33.161948, mean_eps: 0.431457\n",
      " 63399/100000: episode: 209, duration: 1.494s, episode steps: 151, steps per second: 101, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.774907, mae: 22.555261, mean_q: -33.084321, mean_eps: 0.430093\n",
      " 63541/100000: episode: 210, duration: 1.440s, episode steps: 142, steps per second:  99, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.820148, mae: 22.636058, mean_q: -33.189671, mean_eps: 0.428774\n",
      " 63651/100000: episode: 211, duration: 1.059s, episode steps: 110, steps per second: 104, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.236 [0.000, 2.000],  loss: 0.447943, mae: 22.397552, mean_q: -32.886861, mean_eps: 0.427640\n",
      " 63808/100000: episode: 212, duration: 1.460s, episode steps: 157, steps per second: 108, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.788565, mae: 22.604826, mean_q: -33.148935, mean_eps: 0.426439\n",
      " 63976/100000: episode: 213, duration: 1.534s, episode steps: 168, steps per second: 110, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.518966, mae: 22.749525, mean_q: -33.406752, mean_eps: 0.424976\n",
      " 64183/100000: episode: 214, duration: 1.858s, episode steps: 207, steps per second: 111, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.651917, mae: 22.662854, mean_q: -33.233480, mean_eps: 0.423289\n",
      " 64301/100000: episode: 215, duration: 1.067s, episode steps: 118, steps per second: 111, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.623066, mae: 22.748342, mean_q: -33.387676, mean_eps: 0.421826\n",
      " 64408/100000: episode: 216, duration: 1.013s, episode steps: 107, steps per second: 106, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.435801, mae: 22.691269, mean_q: -33.334664, mean_eps: 0.420814\n",
      " 64541/100000: episode: 217, duration: 1.214s, episode steps: 133, steps per second: 110, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.653127, mae: 22.844082, mean_q: -33.507825, mean_eps: 0.419734\n",
      " 64678/100000: episode: 218, duration: 1.335s, episode steps: 137, steps per second: 103, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.797512, mae: 22.666358, mean_q: -33.236998, mean_eps: 0.418519\n",
      " 64833/100000: episode: 219, duration: 1.533s, episode steps: 155, steps per second: 101, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.642676, mae: 22.768953, mean_q: -33.422789, mean_eps: 0.417205\n",
      " 64999/100000: episode: 220, duration: 1.542s, episode steps: 166, steps per second: 108, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.683972, mae: 22.862887, mean_q: -33.569731, mean_eps: 0.415761\n",
      " 65141/100000: episode: 221, duration: 1.293s, episode steps: 142, steps per second: 110, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.554921, mae: 22.684829, mean_q: -33.285517, mean_eps: 0.414374\n",
      " 65298/100000: episode: 222, duration: 1.570s, episode steps: 157, steps per second: 100, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.681912, mae: 22.738081, mean_q: -33.373007, mean_eps: 0.413029\n",
      " 65464/100000: episode: 223, duration: 1.570s, episode steps: 166, steps per second: 106, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.626402, mae: 22.596924, mean_q: -33.179141, mean_eps: 0.411575\n",
      " 65581/100000: episode: 224, duration: 1.067s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.405091, mae: 22.781208, mean_q: -33.450807, mean_eps: 0.410302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65669/100000: episode: 225, duration: 0.876s, episode steps:  88, steps per second: 100, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.396478, mae: 22.625508, mean_q: -33.210300, mean_eps: 0.409380\n",
      " 65802/100000: episode: 226, duration: 1.267s, episode steps: 133, steps per second: 105, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.511719, mae: 22.794805, mean_q: -33.471420, mean_eps: 0.408385\n",
      " 65894/100000: episode: 227, duration: 0.863s, episode steps:  92, steps per second: 107, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.403099, mae: 22.822347, mean_q: -33.549932, mean_eps: 0.407373\n",
      " 66061/100000: episode: 228, duration: 1.625s, episode steps: 167, steps per second: 103, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.636221, mae: 22.829565, mean_q: -33.501799, mean_eps: 0.406207\n",
      " 66208/100000: episode: 229, duration: 1.324s, episode steps: 147, steps per second: 111, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.786617, mae: 22.899178, mean_q: -33.606495, mean_eps: 0.404794\n",
      " 66390/100000: episode: 230, duration: 1.619s, episode steps: 182, steps per second: 112, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.510037, mae: 22.914893, mean_q: -33.657309, mean_eps: 0.403313\n",
      " 66502/100000: episode: 231, duration: 1.049s, episode steps: 112, steps per second: 107, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.756432, mae: 22.870660, mean_q: -33.595668, mean_eps: 0.401991\n",
      " 66645/100000: episode: 232, duration: 1.470s, episode steps: 143, steps per second:  97, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.392134, mae: 22.660448, mean_q: -33.284977, mean_eps: 0.400843\n",
      " 66750/100000: episode: 233, duration: 1.231s, episode steps: 105, steps per second:  85, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.531869, mae: 22.953546, mean_q: -33.679466, mean_eps: 0.399727\n",
      " 66934/100000: episode: 234, duration: 1.931s, episode steps: 184, steps per second:  95, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.583321, mae: 22.821717, mean_q: -33.502431, mean_eps: 0.398426\n",
      " 67066/100000: episode: 235, duration: 1.226s, episode steps: 132, steps per second: 108, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.781226, mae: 22.871819, mean_q: -33.561219, mean_eps: 0.397005\n",
      " 67200/100000: episode: 236, duration: 1.295s, episode steps: 134, steps per second: 103, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.542200, mae: 22.891885, mean_q: -33.634244, mean_eps: 0.395807\n",
      " 67361/100000: episode: 237, duration: 1.478s, episode steps: 161, steps per second: 109, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.680427, mae: 23.062202, mean_q: -33.854362, mean_eps: 0.394480\n",
      " 67579/100000: episode: 238, duration: 1.958s, episode steps: 218, steps per second: 111, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.768054, mae: 22.819234, mean_q: -33.469799, mean_eps: 0.392775\n",
      " 67703/100000: episode: 239, duration: 1.140s, episode steps: 124, steps per second: 109, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.396932, mae: 23.022030, mean_q: -33.807000, mean_eps: 0.391235\n",
      " 67834/100000: episode: 240, duration: 1.219s, episode steps: 131, steps per second: 107, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.727476, mae: 23.052274, mean_q: -33.846037, mean_eps: 0.390088\n",
      " 67960/100000: episode: 241, duration: 1.166s, episode steps: 126, steps per second: 108, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.670262, mae: 22.842466, mean_q: -33.511578, mean_eps: 0.388932\n",
      " 68086/100000: episode: 242, duration: 1.132s, episode steps: 126, steps per second: 111, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 1.004014, mae: 23.209602, mean_q: -34.035749, mean_eps: 0.387797\n",
      " 68229/100000: episode: 243, duration: 1.316s, episode steps: 143, steps per second: 109, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.481662, mae: 23.352702, mean_q: -34.264447, mean_eps: 0.386587\n",
      " 68430/100000: episode: 244, duration: 1.823s, episode steps: 201, steps per second: 110, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.626806, mae: 23.205975, mean_q: -34.034793, mean_eps: 0.385039\n",
      " 68589/100000: episode: 245, duration: 1.455s, episode steps: 159, steps per second: 109, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.644446, mae: 23.225084, mean_q: -34.072668, mean_eps: 0.383419\n",
      " 68746/100000: episode: 246, duration: 1.440s, episode steps: 157, steps per second: 109, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.960973, mae: 23.238617, mean_q: -34.077683, mean_eps: 0.381997\n",
      " 68871/100000: episode: 247, duration: 1.137s, episode steps: 125, steps per second: 110, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.475074, mae: 23.217661, mean_q: -34.086704, mean_eps: 0.380728\n",
      " 68999/100000: episode: 248, duration: 1.157s, episode steps: 128, steps per second: 111, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.587010, mae: 23.161119, mean_q: -33.990698, mean_eps: 0.379590\n",
      " 69123/100000: episode: 249, duration: 1.121s, episode steps: 124, steps per second: 111, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.218 [0.000, 2.000],  loss: 0.768077, mae: 23.318497, mean_q: -34.160734, mean_eps: 0.378455\n",
      " 69247/100000: episode: 250, duration: 1.161s, episode steps: 124, steps per second: 107, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.951428, mae: 23.448643, mean_q: -34.361953, mean_eps: 0.377339\n",
      " 69398/100000: episode: 251, duration: 1.427s, episode steps: 151, steps per second: 106, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.579755, mae: 23.464975, mean_q: -34.417523, mean_eps: 0.376102\n",
      " 69537/100000: episode: 252, duration: 1.248s, episode steps: 139, steps per second: 111, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.718373, mae: 23.349405, mean_q: -34.207119, mean_eps: 0.374797\n",
      " 69703/100000: episode: 253, duration: 1.517s, episode steps: 166, steps per second: 109, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.519931, mae: 23.267829, mean_q: -34.102331, mean_eps: 0.373424\n",
      " 69858/100000: episode: 254, duration: 1.484s, episode steps: 155, steps per second: 104, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.851452, mae: 23.326687, mean_q: -34.199960, mean_eps: 0.371980\n",
      " 70015/100000: episode: 255, duration: 1.450s, episode steps: 157, steps per second: 108, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.686554, mae: 23.405718, mean_q: -34.311303, mean_eps: 0.370576\n",
      " 70144/100000: episode: 256, duration: 1.204s, episode steps: 129, steps per second: 107, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.760422, mae: 23.573028, mean_q: -34.544164, mean_eps: 0.369289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70263/100000: episode: 257, duration: 1.143s, episode steps: 119, steps per second: 104, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.803117, mae: 23.649308, mean_q: -34.659685, mean_eps: 0.368173\n",
      " 70398/100000: episode: 258, duration: 1.307s, episode steps: 135, steps per second: 103, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.689521, mae: 23.657481, mean_q: -34.680564, mean_eps: 0.367030\n",
      " 70544/100000: episode: 259, duration: 1.410s, episode steps: 146, steps per second: 104, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.711167, mae: 23.508777, mean_q: -34.449865, mean_eps: 0.365766\n",
      " 70690/100000: episode: 260, duration: 1.405s, episode steps: 146, steps per second: 104, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.504290, mae: 23.625382, mean_q: -34.647550, mean_eps: 0.364451\n",
      " 70833/100000: episode: 261, duration: 1.396s, episode steps: 143, steps per second: 102, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.538434, mae: 23.348628, mean_q: -34.201362, mean_eps: 0.363151\n",
      " 70982/100000: episode: 262, duration: 1.363s, episode steps: 149, steps per second: 109, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.526269, mae: 23.491304, mean_q: -34.435376, mean_eps: 0.361837\n",
      " 71096/100000: episode: 263, duration: 1.088s, episode steps: 114, steps per second: 105, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.735209, mae: 23.590648, mean_q: -34.558393, mean_eps: 0.360654\n",
      " 71230/100000: episode: 264, duration: 1.263s, episode steps: 134, steps per second: 106, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.637327, mae: 23.662091, mean_q: -34.673063, mean_eps: 0.359538\n",
      " 71349/100000: episode: 265, duration: 1.104s, episode steps: 119, steps per second: 108, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.593914, mae: 23.494569, mean_q: -34.378852, mean_eps: 0.358399\n",
      " 71530/100000: episode: 266, duration: 1.674s, episode steps: 181, steps per second: 108, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.721312, mae: 23.453791, mean_q: -34.361406, mean_eps: 0.357049\n",
      " 71628/100000: episode: 267, duration: 0.941s, episode steps:  98, steps per second: 104, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.538773, mae: 23.680823, mean_q: -34.716472, mean_eps: 0.355793\n",
      " 71766/100000: episode: 268, duration: 1.424s, episode steps: 138, steps per second:  97, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.030698, mae: 23.534369, mean_q: -34.414004, mean_eps: 0.354731\n",
      " 71874/100000: episode: 269, duration: 0.973s, episode steps: 108, steps per second: 111, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.555586, mae: 23.470926, mean_q: -34.391655, mean_eps: 0.353625\n",
      " 72004/100000: episode: 270, duration: 1.177s, episode steps: 130, steps per second: 110, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.708075, mae: 23.521599, mean_q: -34.452136, mean_eps: 0.352553\n",
      " 72123/100000: episode: 271, duration: 1.073s, episode steps: 119, steps per second: 111, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.858486, mae: 23.450113, mean_q: -34.287808, mean_eps: 0.351433\n",
      " 72287/100000: episode: 272, duration: 1.471s, episode steps: 164, steps per second: 112, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.706201, mae: 23.538765, mean_q: -34.458681, mean_eps: 0.350160\n",
      " 72451/100000: episode: 273, duration: 1.592s, episode steps: 164, steps per second: 103, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.567111, mae: 23.473831, mean_q: -34.394163, mean_eps: 0.348684\n",
      " 72559/100000: episode: 274, duration: 1.023s, episode steps: 108, steps per second: 106, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.879232, mae: 23.557930, mean_q: -34.479827, mean_eps: 0.347460\n",
      " 72702/100000: episode: 275, duration: 1.281s, episode steps: 143, steps per second: 112, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.559855, mae: 23.489065, mean_q: -34.402429, mean_eps: 0.346330\n",
      " 72802/100000: episode: 276, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.957156, mae: 23.217349, mean_q: -33.979419, mean_eps: 0.345236\n",
      " 72920/100000: episode: 277, duration: 1.154s, episode steps: 118, steps per second: 102, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.750740, mae: 23.586497, mean_q: -34.556145, mean_eps: 0.344256\n",
      " 73050/100000: episode: 278, duration: 1.242s, episode steps: 130, steps per second: 105, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.226617, mae: 23.571278, mean_q: -34.475493, mean_eps: 0.343140\n",
      " 73169/100000: episode: 279, duration: 1.095s, episode steps: 119, steps per second: 109, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.666555, mae: 23.394116, mean_q: -34.279588, mean_eps: 0.342019\n",
      " 73311/100000: episode: 280, duration: 1.342s, episode steps: 142, steps per second: 106, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.553932, mae: 23.634157, mean_q: -34.628926, mean_eps: 0.340844\n",
      " 73457/100000: episode: 281, duration: 1.364s, episode steps: 146, steps per second: 107, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.623884, mae: 23.403541, mean_q: -34.245575, mean_eps: 0.339548\n",
      " 73624/100000: episode: 282, duration: 1.563s, episode steps: 167, steps per second: 107, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 0.792024, mae: 23.577056, mean_q: -34.539202, mean_eps: 0.338140\n",
      " 73793/100000: episode: 283, duration: 1.550s, episode steps: 169, steps per second: 109, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.436240, mae: 23.552628, mean_q: -34.532024, mean_eps: 0.336628\n",
      " 73897/100000: episode: 284, duration: 0.976s, episode steps: 104, steps per second: 107, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.692666, mae: 23.431381, mean_q: -34.325227, mean_eps: 0.335399\n",
      " 74045/100000: episode: 285, duration: 1.412s, episode steps: 148, steps per second: 105, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.518681, mae: 23.538640, mean_q: -34.475247, mean_eps: 0.334265\n",
      " 74168/100000: episode: 286, duration: 1.159s, episode steps: 123, steps per second: 106, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.984443, mae: 23.551140, mean_q: -34.461742, mean_eps: 0.333046\n",
      " 74301/100000: episode: 287, duration: 1.232s, episode steps: 133, steps per second: 108, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.737322, mae: 23.706113, mean_q: -34.701060, mean_eps: 0.331894\n",
      " 74440/100000: episode: 288, duration: 1.329s, episode steps: 139, steps per second: 105, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.727 [0.000, 2.000],  loss: 1.116424, mae: 23.646524, mean_q: -34.588332, mean_eps: 0.330670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74582/100000: episode: 289, duration: 1.333s, episode steps: 142, steps per second: 107, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 1.102666, mae: 23.676752, mean_q: -34.647054, mean_eps: 0.329406\n",
      " 74695/100000: episode: 290, duration: 1.089s, episode steps: 113, steps per second: 104, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.621460, mae: 23.692871, mean_q: -34.694265, mean_eps: 0.328258\n",
      " 74808/100000: episode: 291, duration: 1.033s, episode steps: 113, steps per second: 109, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.596807, mae: 23.494422, mean_q: -34.402690, mean_eps: 0.327241\n",
      " 74917/100000: episode: 292, duration: 0.999s, episode steps: 109, steps per second: 109, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.617792, mae: 23.638400, mean_q: -34.630194, mean_eps: 0.326242\n",
      " 75063/100000: episode: 293, duration: 1.333s, episode steps: 146, steps per second: 110, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.588428, mae: 23.629177, mean_q: -34.602498, mean_eps: 0.325095\n",
      " 75191/100000: episode: 294, duration: 1.193s, episode steps: 128, steps per second: 107, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.954576, mae: 23.811525, mean_q: -34.821498, mean_eps: 0.323862\n",
      " 75343/100000: episode: 295, duration: 1.543s, episode steps: 152, steps per second:  98, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.836557, mae: 23.780242, mean_q: -34.798145, mean_eps: 0.322601\n",
      " 75451/100000: episode: 296, duration: 1.059s, episode steps: 108, steps per second: 102, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.636614, mae: 23.811873, mean_q: -34.864898, mean_eps: 0.321431\n",
      " 75571/100000: episode: 297, duration: 1.195s, episode steps: 120, steps per second: 100, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.775037, mae: 23.708117, mean_q: -34.689895, mean_eps: 0.320405\n",
      " 75710/100000: episode: 298, duration: 1.386s, episode steps: 139, steps per second: 100, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.598454, mae: 23.654409, mean_q: -34.630873, mean_eps: 0.319240\n",
      " 75827/100000: episode: 299, duration: 1.155s, episode steps: 117, steps per second: 101, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.840838, mae: 23.857845, mean_q: -34.894568, mean_eps: 0.318088\n",
      " 75950/100000: episode: 300, duration: 1.204s, episode steps: 123, steps per second: 102, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.629055, mae: 23.660256, mean_q: -34.618605, mean_eps: 0.317008\n",
      " 76086/100000: episode: 301, duration: 1.275s, episode steps: 136, steps per second: 107, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.801536, mae: 23.765146, mean_q: -34.742833, mean_eps: 0.315843\n",
      " 76179/100000: episode: 302, duration: 0.848s, episode steps:  93, steps per second: 110, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.896806, mae: 23.840895, mean_q: -34.846906, mean_eps: 0.314812\n",
      " 76281/100000: episode: 303, duration: 0.913s, episode steps: 102, steps per second: 112, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.513272, mae: 23.901137, mean_q: -34.955609, mean_eps: 0.313935\n",
      " 76383/100000: episode: 304, duration: 1.006s, episode steps: 102, steps per second: 101, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.633798, mae: 23.843379, mean_q: -34.878258, mean_eps: 0.313017\n",
      " 76506/100000: episode: 305, duration: 1.195s, episode steps: 123, steps per second: 103, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.894422, mae: 23.779117, mean_q: -34.745993, mean_eps: 0.312004\n",
      " 76618/100000: episode: 306, duration: 1.019s, episode steps: 112, steps per second: 110, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.614470, mae: 23.846424, mean_q: -34.881028, mean_eps: 0.310947\n",
      " 76744/100000: episode: 307, duration: 1.143s, episode steps: 126, steps per second: 110, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.668495, mae: 23.745040, mean_q: -34.718390, mean_eps: 0.309876\n",
      " 76857/100000: episode: 308, duration: 1.018s, episode steps: 113, steps per second: 111, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.707889, mae: 23.679982, mean_q: -34.618846, mean_eps: 0.308800\n",
      " 76977/100000: episode: 309, duration: 1.094s, episode steps: 120, steps per second: 110, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.806912, mae: 23.727095, mean_q: -34.684859, mean_eps: 0.307752\n",
      " 77089/100000: episode: 310, duration: 1.132s, episode steps: 112, steps per second:  99, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 0.885531, mae: 23.681576, mean_q: -34.603574, mean_eps: 0.306707\n",
      " 77214/100000: episode: 311, duration: 1.236s, episode steps: 125, steps per second: 101, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.680426, mae: 23.552471, mean_q: -34.428152, mean_eps: 0.305641\n",
      " 77320/100000: episode: 312, duration: 1.000s, episode steps: 106, steps per second: 106, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.620690, mae: 23.672165, mean_q: -34.624108, mean_eps: 0.304602\n",
      " 77438/100000: episode: 313, duration: 1.083s, episode steps: 118, steps per second: 109, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.670537, mae: 23.739484, mean_q: -34.700973, mean_eps: 0.303593\n",
      " 77542/100000: episode: 314, duration: 0.988s, episode steps: 104, steps per second: 105, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.005112, mae: 23.702868, mean_q: -34.650636, mean_eps: 0.302594\n",
      " 77625/100000: episode: 315, duration: 0.762s, episode steps:  83, steps per second: 109, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.612532, mae: 23.413152, mean_q: -34.236811, mean_eps: 0.301753\n",
      " 77716/100000: episode: 316, duration: 0.841s, episode steps:  91, steps per second: 108, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.660506, mae: 23.748234, mean_q: -34.740303, mean_eps: 0.300970\n",
      " 77851/100000: episode: 317, duration: 1.250s, episode steps: 135, steps per second: 108, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.526966, mae: 23.650722, mean_q: -34.580141, mean_eps: 0.299953\n",
      " 77928/100000: episode: 318, duration: 0.743s, episode steps:  77, steps per second: 104, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.615982, mae: 23.774821, mean_q: -34.777205, mean_eps: 0.298999\n",
      " 78069/100000: episode: 319, duration: 1.316s, episode steps: 141, steps per second: 107, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.797764, mae: 23.717068, mean_q: -34.613392, mean_eps: 0.298018\n",
      " 78220/100000: episode: 320, duration: 1.480s, episode steps: 151, steps per second: 102, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.680197, mae: 23.667649, mean_q: -34.568157, mean_eps: 0.296704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78356/100000: episode: 321, duration: 1.293s, episode steps: 136, steps per second: 105, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.549865, mae: 23.651012, mean_q: -34.580557, mean_eps: 0.295412\n",
      " 78465/100000: episode: 322, duration: 0.989s, episode steps: 109, steps per second: 110, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.574060, mae: 23.557056, mean_q: -34.419607, mean_eps: 0.294310\n",
      " 78582/100000: episode: 323, duration: 1.066s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.772263, mae: 23.636330, mean_q: -34.513107, mean_eps: 0.293293\n",
      " 78755/100000: episode: 324, duration: 1.578s, episode steps: 173, steps per second: 110, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.994409, mae: 23.510880, mean_q: -34.343938, mean_eps: 0.291988\n",
      " 78855/100000: episode: 325, duration: 1.016s, episode steps: 100, steps per second:  98, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.976614, mae: 23.571630, mean_q: -34.440314, mean_eps: 0.290760\n",
      " 78983/100000: episode: 326, duration: 1.202s, episode steps: 128, steps per second: 107, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.650571, mae: 23.585011, mean_q: -34.477151, mean_eps: 0.289733\n",
      " 79130/100000: episode: 327, duration: 1.358s, episode steps: 147, steps per second: 108, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.762933, mae: 23.466897, mean_q: -34.252069, mean_eps: 0.288496\n",
      " 79246/100000: episode: 328, duration: 1.062s, episode steps: 116, steps per second: 109, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.164 [0.000, 2.000],  loss: 0.898644, mae: 23.680894, mean_q: -34.609703, mean_eps: 0.287312\n",
      " 79365/100000: episode: 329, duration: 1.088s, episode steps: 119, steps per second: 109, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 1.065934, mae: 23.512817, mean_q: -34.336413, mean_eps: 0.286255\n",
      " 79477/100000: episode: 330, duration: 1.081s, episode steps: 112, steps per second: 104, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.649077, mae: 23.682805, mean_q: -34.684791, mean_eps: 0.285215\n",
      " 79560/100000: episode: 331, duration: 0.815s, episode steps:  83, steps per second: 102, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.519066, mae: 23.648931, mean_q: -34.568820, mean_eps: 0.284338\n",
      " 79652/100000: episode: 332, duration: 0.889s, episode steps:  92, steps per second: 103, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.856073, mae: 23.630798, mean_q: -34.531669, mean_eps: 0.283550\n",
      " 79781/100000: episode: 333, duration: 1.203s, episode steps: 129, steps per second: 107, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.777696, mae: 23.573896, mean_q: -34.448386, mean_eps: 0.282556\n",
      " 79898/100000: episode: 334, duration: 1.140s, episode steps: 117, steps per second: 103, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.967858, mae: 23.389518, mean_q: -34.155164, mean_eps: 0.281449\n",
      " 79975/100000: episode: 335, duration: 0.785s, episode steps:  77, steps per second:  98, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.767659, mae: 23.497577, mean_q: -34.374840, mean_eps: 0.280576\n",
      " 80081/100000: episode: 336, duration: 1.021s, episode steps: 106, steps per second: 104, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.757084, mae: 23.271566, mean_q: -34.001705, mean_eps: 0.279753\n",
      " 80209/100000: episode: 337, duration: 1.313s, episode steps: 128, steps per second:  98, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.846084, mae: 23.557495, mean_q: -34.395444, mean_eps: 0.278699\n",
      " 80328/100000: episode: 338, duration: 1.152s, episode steps: 119, steps per second: 103, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.732824, mae: 23.360086, mean_q: -34.161001, mean_eps: 0.277588\n",
      " 80435/100000: episode: 339, duration: 1.015s, episode steps: 107, steps per second: 105, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.742321, mae: 23.451904, mean_q: -34.302684, mean_eps: 0.276571\n",
      " 80605/100000: episode: 340, duration: 1.642s, episode steps: 170, steps per second: 104, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.859924, mae: 23.487883, mean_q: -34.342684, mean_eps: 0.275325\n",
      " 80709/100000: episode: 341, duration: 0.966s, episode steps: 104, steps per second: 108, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.667214, mae: 23.579985, mean_q: -34.477748, mean_eps: 0.274091\n",
      " 80790/100000: episode: 342, duration: 0.810s, episode steps:  81, steps per second: 100, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.619513, mae: 23.514349, mean_q: -34.403819, mean_eps: 0.273259\n",
      " 80894/100000: episode: 343, duration: 0.968s, episode steps: 104, steps per second: 107, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.849476, mae: 23.443969, mean_q: -34.275266, mean_eps: 0.272426\n",
      " 81030/100000: episode: 344, duration: 1.369s, episode steps: 136, steps per second:  99, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.755653, mae: 23.506052, mean_q: -34.382568, mean_eps: 0.271346\n",
      " 81117/100000: episode: 345, duration: 0.836s, episode steps:  87, steps per second: 104, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.841868, mae: 23.321957, mean_q: -34.037506, mean_eps: 0.270343\n",
      " 81235/100000: episode: 346, duration: 1.090s, episode steps: 118, steps per second: 108, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.899434, mae: 23.449762, mean_q: -34.222808, mean_eps: 0.269420\n",
      " 81329/100000: episode: 347, duration: 0.889s, episode steps:  94, steps per second: 106, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.734298, mae: 23.419059, mean_q: -34.211672, mean_eps: 0.268467\n",
      " 81407/100000: episode: 348, duration: 0.709s, episode steps:  78, steps per second: 110, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.855762, mae: 23.348457, mean_q: -34.125071, mean_eps: 0.267692\n",
      " 81529/100000: episode: 349, duration: 1.120s, episode steps: 122, steps per second: 109, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.656548, mae: 23.504558, mean_q: -34.385364, mean_eps: 0.266792\n",
      " 81617/100000: episode: 350, duration: 0.814s, episode steps:  88, steps per second: 108, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.520395, mae: 23.646363, mean_q: -34.635169, mean_eps: 0.265847\n",
      " 81719/100000: episode: 351, duration: 0.917s, episode steps: 102, steps per second: 111, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.669634, mae: 23.417940, mean_q: -34.224763, mean_eps: 0.264993\n",
      " 81835/100000: episode: 352, duration: 1.059s, episode steps: 116, steps per second: 110, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.827840, mae: 23.479030, mean_q: -34.299770, mean_eps: 0.264012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81924/100000: episode: 353, duration: 0.812s, episode steps:  89, steps per second: 110, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.601118, mae: 23.538307, mean_q: -34.447780, mean_eps: 0.263089\n",
      " 82020/100000: episode: 354, duration: 0.871s, episode steps:  96, steps per second: 110, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.751520, mae: 23.496627, mean_q: -34.366556, mean_eps: 0.262256\n",
      " 82111/100000: episode: 355, duration: 0.850s, episode steps:  91, steps per second: 107, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.820904, mae: 23.563607, mean_q: -34.361670, mean_eps: 0.261415\n",
      " 82205/100000: episode: 356, duration: 0.861s, episode steps:  94, steps per second: 109, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.804194, mae: 23.271504, mean_q: -33.967614, mean_eps: 0.260582\n",
      " 82312/100000: episode: 357, duration: 0.986s, episode steps: 107, steps per second: 108, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.696667, mae: 23.569663, mean_q: -34.437731, mean_eps: 0.259678\n",
      " 82420/100000: episode: 358, duration: 1.006s, episode steps: 108, steps per second: 107, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.702651, mae: 23.368245, mean_q: -34.070170, mean_eps: 0.258711\n",
      " 82536/100000: episode: 359, duration: 1.058s, episode steps: 116, steps per second: 110, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.784886, mae: 23.388522, mean_q: -34.128481, mean_eps: 0.257702\n",
      " 82648/100000: episode: 360, duration: 1.030s, episode steps: 112, steps per second: 109, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.846841, mae: 23.445662, mean_q: -34.195218, mean_eps: 0.256676\n",
      " 82726/100000: episode: 361, duration: 0.717s, episode steps:  78, steps per second: 109, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.872188, mae: 23.275210, mean_q: -34.016191, mean_eps: 0.255821\n",
      " 82819/100000: episode: 362, duration: 0.863s, episode steps:  93, steps per second: 108, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.847807, mae: 23.434113, mean_q: -34.200606, mean_eps: 0.255052\n",
      " 82897/100000: episode: 363, duration: 0.709s, episode steps:  78, steps per second: 110, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.939582, mae: 23.384481, mean_q: -34.144779, mean_eps: 0.254282\n",
      " 83009/100000: episode: 364, duration: 1.025s, episode steps: 112, steps per second: 109, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.577017, mae: 23.331568, mean_q: -34.070190, mean_eps: 0.253427\n",
      " 83126/100000: episode: 365, duration: 1.067s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.584961, mae: 23.459949, mean_q: -34.232844, mean_eps: 0.252397\n",
      " 83246/100000: episode: 366, duration: 1.101s, episode steps: 120, steps per second: 109, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.702542, mae: 23.372446, mean_q: -34.072586, mean_eps: 0.251330\n",
      " 83325/100000: episode: 367, duration: 0.735s, episode steps:  79, steps per second: 107, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.864515, mae: 23.303899, mean_q: -33.959968, mean_eps: 0.250435\n",
      " 83430/100000: episode: 368, duration: 0.996s, episode steps: 105, steps per second: 105, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.867184, mae: 23.427539, mean_q: -34.151256, mean_eps: 0.249607\n",
      " 83547/100000: episode: 369, duration: 1.102s, episode steps: 117, steps per second: 106, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.580708, mae: 23.437425, mean_q: -34.187805, mean_eps: 0.248608\n",
      " 83654/100000: episode: 370, duration: 0.977s, episode steps: 107, steps per second: 110, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.495079, mae: 23.353815, mean_q: -34.056600, mean_eps: 0.247600\n",
      " 83727/100000: episode: 371, duration: 0.679s, episode steps:  73, steps per second: 108, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.247 [0.000, 2.000],  loss: 1.044873, mae: 23.276709, mean_q: -33.938170, mean_eps: 0.246790\n",
      " 83820/100000: episode: 372, duration: 0.875s, episode steps:  93, steps per second: 106, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.611706, mae: 23.241272, mean_q: -33.881101, mean_eps: 0.246043\n",
      " 83952/100000: episode: 373, duration: 1.210s, episode steps: 132, steps per second: 109, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.888970, mae: 23.182009, mean_q: -33.810608, mean_eps: 0.245030\n",
      " 84033/100000: episode: 374, duration: 0.747s, episode steps:  81, steps per second: 108, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.641344, mae: 23.437449, mean_q: -34.193426, mean_eps: 0.244072\n",
      " 84130/100000: episode: 375, duration: 0.871s, episode steps:  97, steps per second: 111, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.919056, mae: 23.345488, mean_q: -34.021465, mean_eps: 0.243271\n",
      " 84232/100000: episode: 376, duration: 0.959s, episode steps: 102, steps per second: 106, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.872616, mae: 23.329905, mean_q: -34.005170, mean_eps: 0.242376\n",
      " 84337/100000: episode: 377, duration: 0.982s, episode steps: 105, steps per second: 107, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.783622, mae: 23.413075, mean_q: -34.134612, mean_eps: 0.241444\n",
      " 84439/100000: episode: 378, duration: 0.934s, episode steps: 102, steps per second: 109, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.765683, mae: 23.368214, mean_q: -34.066800, mean_eps: 0.240512\n",
      " 84544/100000: episode: 379, duration: 0.963s, episode steps: 105, steps per second: 109, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.789359, mae: 23.364782, mean_q: -34.048398, mean_eps: 0.239581\n",
      " 84657/100000: episode: 380, duration: 1.037s, episode steps: 113, steps per second: 109, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.843110, mae: 23.322042, mean_q: -34.041583, mean_eps: 0.238600\n",
      " 84772/100000: episode: 381, duration: 1.063s, episode steps: 115, steps per second: 108, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.691335, mae: 23.363905, mean_q: -34.062444, mean_eps: 0.237574\n",
      " 84869/100000: episode: 382, duration: 0.884s, episode steps:  97, steps per second: 110, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.661078, mae: 23.318217, mean_q: -33.997774, mean_eps: 0.236620\n",
      " 84972/100000: episode: 383, duration: 0.930s, episode steps: 103, steps per second: 111, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.664974, mae: 23.283986, mean_q: -33.973183, mean_eps: 0.235720\n",
      " 85054/100000: episode: 384, duration: 0.752s, episode steps:  82, steps per second: 109, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.905325, mae: 23.235762, mean_q: -33.862374, mean_eps: 0.234887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85146/100000: episode: 385, duration: 0.858s, episode steps:  92, steps per second: 107, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.973968, mae: 23.432453, mean_q: -34.143557, mean_eps: 0.234105\n",
      " 85252/100000: episode: 386, duration: 0.974s, episode steps: 106, steps per second: 109, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.539356, mae: 23.460287, mean_q: -34.232323, mean_eps: 0.233213\n",
      " 85337/100000: episode: 387, duration: 0.878s, episode steps:  85, steps per second:  97, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.612890, mae: 23.482287, mean_q: -34.274154, mean_eps: 0.232354\n",
      " 85445/100000: episode: 388, duration: 0.989s, episode steps: 108, steps per second: 109, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.904497, mae: 23.238221, mean_q: -33.894168, mean_eps: 0.231485\n",
      " 85530/100000: episode: 389, duration: 0.786s, episode steps:  85, steps per second: 108, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.503235, mae: 23.526760, mean_q: -34.359778, mean_eps: 0.230617\n",
      " 85626/100000: episode: 390, duration: 0.877s, episode steps:  96, steps per second: 110, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.561853, mae: 23.293687, mean_q: -33.960503, mean_eps: 0.229802\n",
      " 85727/100000: episode: 391, duration: 0.939s, episode steps: 101, steps per second: 108, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.597532, mae: 23.548655, mean_q: -34.348768, mean_eps: 0.228916\n",
      " 85863/100000: episode: 392, duration: 1.250s, episode steps: 136, steps per second: 109, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.784397, mae: 23.233803, mean_q: -33.860058, mean_eps: 0.227849\n",
      " 85954/100000: episode: 393, duration: 0.831s, episode steps:  91, steps per second: 110, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.592118, mae: 23.187842, mean_q: -33.824058, mean_eps: 0.226828\n",
      " 86031/100000: episode: 394, duration: 0.716s, episode steps:  77, steps per second: 108, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.630896, mae: 23.336065, mean_q: -34.046226, mean_eps: 0.226072\n",
      " 86152/100000: episode: 395, duration: 1.103s, episode steps: 121, steps per second: 110, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.693259, mae: 23.355827, mean_q: -34.040940, mean_eps: 0.225181\n",
      " 86261/100000: episode: 396, duration: 1.026s, episode steps: 109, steps per second: 106, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.779169, mae: 23.261227, mean_q: -33.945132, mean_eps: 0.224146\n",
      " 86361/100000: episode: 397, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.512880, mae: 23.382120, mean_q: -34.149162, mean_eps: 0.223206\n",
      " 86468/100000: episode: 398, duration: 0.974s, episode steps: 107, steps per second: 110, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.722610, mae: 23.410301, mean_q: -34.128952, mean_eps: 0.222274\n",
      " 86571/100000: episode: 399, duration: 0.944s, episode steps: 103, steps per second: 109, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.667309, mae: 23.272601, mean_q: -33.953008, mean_eps: 0.221329\n",
      " 86658/100000: episode: 400, duration: 0.811s, episode steps:  87, steps per second: 107, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.753780, mae: 23.292409, mean_q: -33.955418, mean_eps: 0.220474\n",
      " 86750/100000: episode: 401, duration: 0.847s, episode steps:  92, steps per second: 109, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.611588, mae: 23.303239, mean_q: -34.005406, mean_eps: 0.219669\n",
      " 86865/100000: episode: 402, duration: 1.057s, episode steps: 115, steps per second: 109, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.557233, mae: 23.183728, mean_q: -33.808741, mean_eps: 0.218737\n",
      " 86953/100000: episode: 403, duration: 0.817s, episode steps:  88, steps per second: 108, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.943450, mae: 23.360536, mean_q: -34.044537, mean_eps: 0.217823\n",
      " 87063/100000: episode: 404, duration: 1.011s, episode steps: 110, steps per second: 109, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.885974, mae: 23.211118, mean_q: -33.821878, mean_eps: 0.216932\n",
      " 87183/100000: episode: 405, duration: 1.102s, episode steps: 120, steps per second: 109, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.628049, mae: 23.176364, mean_q: -33.797787, mean_eps: 0.215897\n",
      " 87284/100000: episode: 406, duration: 0.937s, episode steps: 101, steps per second: 108, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.168 [0.000, 2.000],  loss: 0.652030, mae: 23.288150, mean_q: -33.977186, mean_eps: 0.214903\n",
      " 87396/100000: episode: 407, duration: 1.006s, episode steps: 112, steps per second: 111, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.682526, mae: 23.316861, mean_q: -34.018112, mean_eps: 0.213944\n",
      " 87510/100000: episode: 408, duration: 1.037s, episode steps: 114, steps per second: 110, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.595530, mae: 23.044803, mean_q: -33.604769, mean_eps: 0.212927\n",
      " 87601/100000: episode: 409, duration: 0.835s, episode steps:  91, steps per second: 109, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.654809, mae: 23.400747, mean_q: -34.118042, mean_eps: 0.212005\n",
      " 87692/100000: episode: 410, duration: 0.865s, episode steps:  91, steps per second: 105, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.541890, mae: 23.218285, mean_q: -33.885325, mean_eps: 0.211186\n",
      " 87793/100000: episode: 411, duration: 0.945s, episode steps: 101, steps per second: 107, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.817750, mae: 23.267169, mean_q: -33.913968, mean_eps: 0.210322\n",
      " 87907/100000: episode: 412, duration: 1.039s, episode steps: 114, steps per second: 110, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.795350, mae: 23.246326, mean_q: -33.890387, mean_eps: 0.209354\n",
      " 87986/100000: episode: 413, duration: 0.729s, episode steps:  79, steps per second: 108, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.761782, mae: 23.310473, mean_q: -33.984423, mean_eps: 0.208486\n",
      " 88069/100000: episode: 414, duration: 0.761s, episode steps:  83, steps per second: 109, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.992891, mae: 23.231093, mean_q: -33.867587, mean_eps: 0.207757\n",
      " 88161/100000: episode: 415, duration: 0.835s, episode steps:  92, steps per second: 110, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.866905, mae: 23.319924, mean_q: -33.998330, mean_eps: 0.206970\n",
      " 88278/100000: episode: 416, duration: 1.061s, episode steps: 117, steps per second: 110, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.633943, mae: 23.543372, mean_q: -34.347266, mean_eps: 0.206029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88354/100000: episode: 417, duration: 0.700s, episode steps:  76, steps per second: 109, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.550958, mae: 23.271713, mean_q: -33.964397, mean_eps: 0.205160\n",
      " 88478/100000: episode: 418, duration: 1.122s, episode steps: 124, steps per second: 110, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.833453, mae: 23.316612, mean_q: -33.986231, mean_eps: 0.204261\n",
      " 88589/100000: episode: 419, duration: 1.012s, episode steps: 111, steps per second: 110, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.685117, mae: 23.115153, mean_q: -33.711527, mean_eps: 0.203203\n",
      " 88729/100000: episode: 420, duration: 1.280s, episode steps: 140, steps per second: 109, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.720339, mae: 23.285840, mean_q: -33.975312, mean_eps: 0.202073\n",
      " 88844/100000: episode: 421, duration: 1.060s, episode steps: 115, steps per second: 109, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.750889, mae: 23.337821, mean_q: -34.024124, mean_eps: 0.200926\n",
      " 88954/100000: episode: 422, duration: 1.006s, episode steps: 110, steps per second: 109, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.528954, mae: 23.385684, mean_q: -34.130381, mean_eps: 0.199914\n",
      " 89043/100000: episode: 423, duration: 0.831s, episode steps:  89, steps per second: 107, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.785641, mae: 23.475380, mean_q: -34.206475, mean_eps: 0.199018\n",
      " 89151/100000: episode: 424, duration: 0.983s, episode steps: 108, steps per second: 110, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.760107, mae: 23.275836, mean_q: -33.934280, mean_eps: 0.198131\n",
      " 89248/100000: episode: 425, duration: 0.878s, episode steps:  97, steps per second: 110, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.535988, mae: 23.518305, mean_q: -34.330152, mean_eps: 0.197209\n",
      " 89355/100000: episode: 426, duration: 0.974s, episode steps: 107, steps per second: 110, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.740913, mae: 23.387428, mean_q: -34.112817, mean_eps: 0.196291\n",
      " 89432/100000: episode: 427, duration: 0.700s, episode steps:  77, steps per second: 110, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.988089, mae: 23.234774, mean_q: -33.859827, mean_eps: 0.195463\n",
      " 89533/100000: episode: 428, duration: 0.916s, episode steps: 101, steps per second: 110, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.676974, mae: 23.373485, mean_q: -34.116466, mean_eps: 0.194662\n",
      " 89644/100000: episode: 429, duration: 1.017s, episode steps: 111, steps per second: 109, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.732177, mae: 23.397804, mean_q: -34.179427, mean_eps: 0.193708\n",
      " 89743/100000: episode: 430, duration: 0.910s, episode steps:  99, steps per second: 109, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.587912, mae: 23.340731, mean_q: -34.075478, mean_eps: 0.192763\n",
      " 89836/100000: episode: 431, duration: 0.852s, episode steps:  93, steps per second: 109, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.503809, mae: 23.304250, mean_q: -33.989651, mean_eps: 0.191899\n",
      " 89920/100000: episode: 432, duration: 0.762s, episode steps:  84, steps per second: 110, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.627882, mae: 23.393768, mean_q: -34.145133, mean_eps: 0.191102\n",
      " 90003/100000: episode: 433, duration: 0.762s, episode steps:  83, steps per second: 109, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.679538, mae: 23.361689, mean_q: -34.065177, mean_eps: 0.190351\n",
      " 90115/100000: episode: 434, duration: 1.013s, episode steps: 112, steps per second: 111, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.779276, mae: 23.429197, mean_q: -34.140630, mean_eps: 0.189473\n",
      " 90218/100000: episode: 435, duration: 0.935s, episode steps: 103, steps per second: 110, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.272 [0.000, 2.000],  loss: 0.689280, mae: 23.564528, mean_q: -34.362433, mean_eps: 0.188506\n",
      " 90340/100000: episode: 436, duration: 1.114s, episode steps: 122, steps per second: 110, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.689276, mae: 23.662099, mean_q: -34.550336, mean_eps: 0.187493\n",
      " 90425/100000: episode: 437, duration: 0.780s, episode steps:  85, steps per second: 109, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.986777, mae: 23.527685, mean_q: -34.312755, mean_eps: 0.186562\n",
      " 90532/100000: episode: 438, duration: 0.995s, episode steps: 107, steps per second: 108, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.561701, mae: 23.476517, mean_q: -34.281373, mean_eps: 0.185698\n",
      " 90615/100000: episode: 439, duration: 0.759s, episode steps:  83, steps per second: 109, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.958309, mae: 23.483134, mean_q: -34.215427, mean_eps: 0.184843\n",
      " 90712/100000: episode: 440, duration: 0.885s, episode steps:  97, steps per second: 110, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.522748, mae: 23.475800, mean_q: -34.247236, mean_eps: 0.184033\n",
      " 90802/100000: episode: 441, duration: 0.830s, episode steps:  90, steps per second: 108, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.878331, mae: 23.524272, mean_q: -34.316335, mean_eps: 0.183192\n",
      " 90905/100000: episode: 442, duration: 0.931s, episode steps: 103, steps per second: 111, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.610097, mae: 23.550805, mean_q: -34.334643, mean_eps: 0.182323\n",
      " 90987/100000: episode: 443, duration: 0.771s, episode steps:  82, steps per second: 106, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.546451, mae: 23.316155, mean_q: -33.996051, mean_eps: 0.181490\n",
      " 91086/100000: episode: 444, duration: 0.943s, episode steps:  99, steps per second: 105, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.617505, mae: 23.605829, mean_q: -34.369261, mean_eps: 0.180676\n",
      " 91177/100000: episode: 445, duration: 0.824s, episode steps:  91, steps per second: 110, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.695219, mae: 23.541396, mean_q: -34.304094, mean_eps: 0.179821\n",
      " 91277/100000: episode: 446, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.578228, mae: 23.488141, mean_q: -34.241130, mean_eps: 0.178961\n",
      " 91388/100000: episode: 447, duration: 1.010s, episode steps: 111, steps per second: 110, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.746847, mae: 23.351844, mean_q: -34.001480, mean_eps: 0.178012\n",
      " 91503/100000: episode: 448, duration: 1.052s, episode steps: 115, steps per second: 109, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.722341, mae: 23.521398, mean_q: -34.275869, mean_eps: 0.176995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91601/100000: episode: 449, duration: 0.907s, episode steps:  98, steps per second: 108, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.578705, mae: 23.497506, mean_q: -34.243254, mean_eps: 0.176037\n",
      " 91691/100000: episode: 450, duration: 0.831s, episode steps:  90, steps per second: 108, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.655232, mae: 23.432426, mean_q: -34.146562, mean_eps: 0.175190\n",
      " 91811/100000: episode: 451, duration: 1.090s, episode steps: 120, steps per second: 110, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.750221, mae: 23.185668, mean_q: -33.740194, mean_eps: 0.174245\n",
      " 91913/100000: episode: 452, duration: 1.016s, episode steps: 102, steps per second: 100, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.842810, mae: 23.373365, mean_q: -34.005209, mean_eps: 0.173246\n",
      " 91999/100000: episode: 453, duration: 0.782s, episode steps:  86, steps per second: 110, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.612951, mae: 23.580489, mean_q: -34.418507, mean_eps: 0.172400\n",
      " 92120/100000: episode: 454, duration: 1.109s, episode steps: 121, steps per second: 109, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.088710, mae: 23.539291, mean_q: -34.285887, mean_eps: 0.171469\n",
      " 92325/100000: episode: 455, duration: 1.837s, episode steps: 205, steps per second: 112, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.613071, mae: 23.583618, mean_q: -34.350163, mean_eps: 0.170002\n",
      " 92405/100000: episode: 456, duration: 0.730s, episode steps:  80, steps per second: 110, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.722735, mae: 23.564197, mean_q: -34.313998, mean_eps: 0.168719\n",
      " 92489/100000: episode: 457, duration: 0.771s, episode steps:  84, steps per second: 109, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.742829, mae: 23.203281, mean_q: -33.750921, mean_eps: 0.167982\n",
      " 92582/100000: episode: 458, duration: 0.869s, episode steps:  93, steps per second: 107, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.930767, mae: 23.421182, mean_q: -34.075479, mean_eps: 0.167185\n",
      " 92666/100000: episode: 459, duration: 0.808s, episode steps:  84, steps per second: 104, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.634635, mae: 23.388023, mean_q: -34.060977, mean_eps: 0.166388\n",
      " 92765/100000: episode: 460, duration: 0.918s, episode steps:  99, steps per second: 108, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.827495, mae: 23.188715, mean_q: -33.757395, mean_eps: 0.165565\n",
      " 92856/100000: episode: 461, duration: 0.849s, episode steps:  91, steps per second: 107, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.726099, mae: 23.313748, mean_q: -33.938522, mean_eps: 0.164710\n",
      " 92961/100000: episode: 462, duration: 0.963s, episode steps: 105, steps per second: 109, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.550437, mae: 23.482821, mean_q: -34.205330, mean_eps: 0.163828\n",
      " 93055/100000: episode: 463, duration: 0.868s, episode steps:  94, steps per second: 108, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.672594, mae: 23.677187, mean_q: -34.481558, mean_eps: 0.162932\n",
      " 93139/100000: episode: 464, duration: 0.770s, episode steps:  84, steps per second: 109, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.787693, mae: 23.610421, mean_q: -34.379000, mean_eps: 0.162131\n",
      " 93235/100000: episode: 465, duration: 0.879s, episode steps:  96, steps per second: 109, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.708460, mae: 23.452608, mean_q: -34.123737, mean_eps: 0.161321\n",
      " 93344/100000: episode: 466, duration: 0.989s, episode steps: 109, steps per second: 110, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.641867, mae: 23.493157, mean_q: -34.210043, mean_eps: 0.160399\n",
      " 93414/100000: episode: 467, duration: 0.647s, episode steps:  70, steps per second: 108, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.540045, mae: 23.497276, mean_q: -34.232411, mean_eps: 0.159593\n",
      " 93499/100000: episode: 468, duration: 0.773s, episode steps:  85, steps per second: 110, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.792562, mae: 23.381549, mean_q: -34.024165, mean_eps: 0.158896\n",
      " 93596/100000: episode: 469, duration: 0.888s, episode steps:  97, steps per second: 109, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.629842, mae: 23.545258, mean_q: -34.294349, mean_eps: 0.158077\n",
      " 93664/100000: episode: 470, duration: 0.642s, episode steps:  68, steps per second: 106, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.865962, mae: 23.656072, mean_q: -34.421405, mean_eps: 0.157334\n",
      " 93760/100000: episode: 471, duration: 0.898s, episode steps:  96, steps per second: 107, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.658392, mae: 23.644485, mean_q: -34.470861, mean_eps: 0.156596\n",
      " 93867/100000: episode: 472, duration: 0.968s, episode steps: 107, steps per second: 111, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.562396, mae: 23.513088, mean_q: -34.238237, mean_eps: 0.155683\n",
      " 93951/100000: episode: 473, duration: 0.764s, episode steps:  84, steps per second: 110, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.729799, mae: 23.461650, mean_q: -34.191906, mean_eps: 0.154823\n",
      " 94072/100000: episode: 474, duration: 1.109s, episode steps: 121, steps per second: 109, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.782160, mae: 23.508708, mean_q: -34.196563, mean_eps: 0.153901\n",
      " 94175/100000: episode: 475, duration: 0.928s, episode steps: 103, steps per second: 111, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.688049, mae: 23.421560, mean_q: -34.082235, mean_eps: 0.152893\n",
      " 94295/100000: episode: 476, duration: 1.103s, episode steps: 120, steps per second: 109, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.692131, mae: 23.259510, mean_q: -33.850573, mean_eps: 0.151889\n",
      " 94396/100000: episode: 477, duration: 0.925s, episode steps: 101, steps per second: 109, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.633273, mae: 23.334784, mean_q: -33.963469, mean_eps: 0.150895\n",
      " 94503/100000: episode: 478, duration: 0.979s, episode steps: 107, steps per second: 109, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.614432, mae: 23.266289, mean_q: -33.848347, mean_eps: 0.149959\n",
      " 94586/100000: episode: 479, duration: 0.760s, episode steps:  83, steps per second: 109, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.618407, mae: 23.330974, mean_q: -33.920403, mean_eps: 0.149104\n",
      " 94662/100000: episode: 480, duration: 0.708s, episode steps:  76, steps per second: 107, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.742801, mae: 23.224160, mean_q: -33.775615, mean_eps: 0.148388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94758/100000: episode: 481, duration: 0.870s, episode steps:  96, steps per second: 110, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.617019, mae: 23.364202, mean_q: -33.997123, mean_eps: 0.147614\n",
      " 94873/100000: episode: 482, duration: 1.048s, episode steps: 115, steps per second: 110, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.767955, mae: 23.325846, mean_q: -33.976205, mean_eps: 0.146665\n",
      " 94984/100000: episode: 483, duration: 1.009s, episode steps: 111, steps per second: 110, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.527873, mae: 23.438899, mean_q: -34.152283, mean_eps: 0.145648\n",
      " 95098/100000: episode: 484, duration: 1.013s, episode steps: 114, steps per second: 113, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.660788, mae: 23.306799, mean_q: -33.887989, mean_eps: 0.144635\n",
      " 95251/100000: episode: 485, duration: 1.394s, episode steps: 153, steps per second: 110, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.671246, mae: 23.180317, mean_q: -33.705841, mean_eps: 0.143434\n",
      " 95327/100000: episode: 486, duration: 0.715s, episode steps:  76, steps per second: 106, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.778587, mae: 23.416987, mean_q: -34.030499, mean_eps: 0.142403\n",
      " 95415/100000: episode: 487, duration: 0.827s, episode steps:  88, steps per second: 106, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.675807, mae: 23.311223, mean_q: -33.909396, mean_eps: 0.141665\n",
      " 95504/100000: episode: 488, duration: 0.833s, episode steps:  89, steps per second: 107, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.748535, mae: 23.293184, mean_q: -33.877352, mean_eps: 0.140869\n",
      " 95659/100000: episode: 489, duration: 1.424s, episode steps: 155, steps per second: 109, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.688547, mae: 23.318852, mean_q: -33.933381, mean_eps: 0.139771\n",
      " 95773/100000: episode: 490, duration: 1.027s, episode steps: 114, steps per second: 111, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.558651, mae: 23.266387, mean_q: -33.849375, mean_eps: 0.138561\n",
      " 95877/100000: episode: 491, duration: 0.956s, episode steps: 104, steps per second: 109, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.641493, mae: 23.240558, mean_q: -33.814285, mean_eps: 0.137579\n",
      " 95955/100000: episode: 492, duration: 0.702s, episode steps:  78, steps per second: 111, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.676126, mae: 23.234244, mean_q: -33.795190, mean_eps: 0.136760\n",
      " 96056/100000: episode: 493, duration: 0.930s, episode steps: 101, steps per second: 109, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.670814, mae: 23.086040, mean_q: -33.579211, mean_eps: 0.135955\n",
      " 96127/100000: episode: 494, duration: 0.672s, episode steps:  71, steps per second: 106, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.675296, mae: 23.243675, mean_q: -33.803337, mean_eps: 0.135181\n",
      " 96228/100000: episode: 495, duration: 0.909s, episode steps: 101, steps per second: 111, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.695604, mae: 23.127279, mean_q: -33.591953, mean_eps: 0.134407\n",
      " 96308/100000: episode: 496, duration: 0.718s, episode steps:  80, steps per second: 111, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.786432, mae: 23.337142, mean_q: -33.949392, mean_eps: 0.133592\n",
      " 96410/100000: episode: 497, duration: 0.934s, episode steps: 102, steps per second: 109, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.683242, mae: 23.315421, mean_q: -33.910420, mean_eps: 0.132773\n",
      " 96507/100000: episode: 498, duration: 0.906s, episode steps:  97, steps per second: 107, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.648251, mae: 23.054722, mean_q: -33.494292, mean_eps: 0.131878\n",
      " 96585/100000: episode: 499, duration: 0.713s, episode steps:  78, steps per second: 109, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.606772, mae: 23.410578, mean_q: -34.077578, mean_eps: 0.131090\n",
      " 96666/100000: episode: 500, duration: 0.730s, episode steps:  81, steps per second: 111, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.606575, mae: 23.115359, mean_q: -33.579598, mean_eps: 0.130375\n",
      " 96754/100000: episode: 501, duration: 0.805s, episode steps:  88, steps per second: 109, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.766859, mae: 23.198256, mean_q: -33.687597, mean_eps: 0.129614\n",
      " 96853/100000: episode: 502, duration: 0.900s, episode steps:  99, steps per second: 110, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.655770, mae: 23.191902, mean_q: -33.690349, mean_eps: 0.128773\n",
      " 96949/100000: episode: 503, duration: 0.870s, episode steps:  96, steps per second: 110, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.618578, mae: 23.247004, mean_q: -33.816217, mean_eps: 0.127895\n",
      " 97049/100000: episode: 504, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.610081, mae: 23.374730, mean_q: -34.022468, mean_eps: 0.127013\n",
      " 97130/100000: episode: 505, duration: 0.733s, episode steps:  81, steps per second: 111, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.675141, mae: 23.249480, mean_q: -33.776705, mean_eps: 0.126199\n",
      " 97213/100000: episode: 506, duration: 0.740s, episode steps:  83, steps per second: 112, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.653582, mae: 23.070007, mean_q: -33.476213, mean_eps: 0.125461\n",
      " 97318/100000: episode: 507, duration: 0.957s, episode steps: 105, steps per second: 110, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.762430, mae: 23.299351, mean_q: -33.852954, mean_eps: 0.124615\n",
      " 97424/100000: episode: 508, duration: 0.962s, episode steps: 106, steps per second: 110, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.628957, mae: 23.186078, mean_q: -33.702211, mean_eps: 0.123665\n",
      " 97517/100000: episode: 509, duration: 0.870s, episode steps:  93, steps per second: 107, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.657177, mae: 23.106889, mean_q: -33.589398, mean_eps: 0.122770\n",
      " 97598/100000: episode: 510, duration: 0.737s, episode steps:  81, steps per second: 110, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.653770, mae: 23.041723, mean_q: -33.483750, mean_eps: 0.121987\n",
      " 97675/100000: episode: 511, duration: 0.698s, episode steps:  77, steps per second: 110, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.625186, mae: 23.155621, mean_q: -33.615735, mean_eps: 0.121276\n",
      " 97762/100000: episode: 512, duration: 0.799s, episode steps:  87, steps per second: 109, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.635006, mae: 23.159988, mean_q: -33.681749, mean_eps: 0.120538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97873/100000: episode: 513, duration: 1.008s, episode steps: 111, steps per second: 110, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.616266, mae: 23.320604, mean_q: -33.931726, mean_eps: 0.119647\n",
      " 97971/100000: episode: 514, duration: 0.896s, episode steps:  98, steps per second: 109, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.537007, mae: 23.022543, mean_q: -33.466717, mean_eps: 0.118706\n",
      " 98048/100000: episode: 515, duration: 0.703s, episode steps:  77, steps per second: 110, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.698841, mae: 23.059611, mean_q: -33.508925, mean_eps: 0.117919\n",
      " 98148/100000: episode: 516, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.760742, mae: 22.885742, mean_q: -33.219593, mean_eps: 0.117122\n",
      " 98242/100000: episode: 517, duration: 0.866s, episode steps:  94, steps per second: 109, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 0.665438, mae: 23.124457, mean_q: -33.564548, mean_eps: 0.116249\n",
      " 98347/100000: episode: 518, duration: 0.971s, episode steps: 105, steps per second: 108, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.642826, mae: 22.991548, mean_q: -33.391969, mean_eps: 0.115354\n",
      " 98432/100000: episode: 519, duration: 0.766s, episode steps:  85, steps per second: 111, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.736006, mae: 23.029900, mean_q: -33.473934, mean_eps: 0.114499\n",
      " 98542/100000: episode: 520, duration: 1.097s, episode steps: 110, steps per second: 100, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.596909, mae: 22.940497, mean_q: -33.282475, mean_eps: 0.113621\n",
      " 98643/100000: episode: 521, duration: 0.948s, episode steps: 101, steps per second: 107, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.628849, mae: 22.958410, mean_q: -33.290583, mean_eps: 0.112672\n",
      " 98737/100000: episode: 522, duration: 0.846s, episode steps:  94, steps per second: 111, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.582197, mae: 22.778359, mean_q: -33.064816, mean_eps: 0.111794\n",
      " 98844/100000: episode: 523, duration: 1.010s, episode steps: 107, steps per second: 106, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.675523, mae: 22.715785, mean_q: -32.963657, mean_eps: 0.110890\n",
      " 98912/100000: episode: 524, duration: 0.633s, episode steps:  68, steps per second: 107, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.641911, mae: 22.896808, mean_q: -33.252920, mean_eps: 0.110102\n",
      " 98993/100000: episode: 525, duration: 0.759s, episode steps:  81, steps per second: 107, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.601609, mae: 23.103058, mean_q: -33.565276, mean_eps: 0.109432\n",
      " 99118/100000: episode: 526, duration: 1.139s, episode steps: 125, steps per second: 110, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.678206, mae: 22.962165, mean_q: -33.356926, mean_eps: 0.108505\n",
      " 99205/100000: episode: 527, duration: 0.803s, episode steps:  87, steps per second: 108, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.612582, mae: 23.116751, mean_q: -33.577350, mean_eps: 0.107551\n",
      " 99280/100000: episode: 528, duration: 0.686s, episode steps:  75, steps per second: 109, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.666648, mae: 22.850514, mean_q: -33.165589, mean_eps: 0.106822\n",
      " 99364/100000: episode: 529, duration: 0.760s, episode steps:  84, steps per second: 111, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.636812, mae: 22.752116, mean_q: -33.033655, mean_eps: 0.106106\n",
      " 99460/100000: episode: 530, duration: 0.864s, episode steps:  96, steps per second: 111, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.627671, mae: 22.705757, mean_q: -32.979352, mean_eps: 0.105296\n",
      " 99550/100000: episode: 531, duration: 0.834s, episode steps:  90, steps per second: 108, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.592093, mae: 23.180552, mean_q: -33.690549, mean_eps: 0.104459\n",
      " 99647/100000: episode: 532, duration: 0.999s, episode steps:  97, steps per second:  97, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.541254, mae: 22.961032, mean_q: -33.383031, mean_eps: 0.103618\n",
      " 99758/100000: episode: 533, duration: 1.137s, episode steps: 111, steps per second:  98, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.631029, mae: 22.748614, mean_q: -32.997589, mean_eps: 0.102682\n",
      " 99858/100000: episode: 534, duration: 1.035s, episode steps: 100, steps per second:  97, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.641320, mae: 22.966970, mean_q: -33.304320, mean_eps: 0.101732\n",
      " 99958/100000: episode: 535, duration: 1.034s, episode steps: 100, steps per second:  97, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.579430, mae: 22.820250, mean_q: -33.168605, mean_eps: 0.100832\n",
      "done, took 892.644 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24ec1d95c88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100_000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "framed-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -77.000, steps: 78\n",
      "Episode 2: reward: -69.000, steps: 70\n",
      "Episode 3: reward: -87.000, steps: 88\n",
      "Episode 4: reward: -77.000, steps: 78\n",
      "Episode 5: reward: -77.000, steps: 78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24ec1ed2c48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
